Different possibilities of analysis:
Target: Precip, Features:SST

Precip "resolution", how many models do we fit?:
for each grid one model, ncell models
for each cluster one model, ncluster models
for the CAB one model, 1 model

Raw data or anomalies?

STL, Decomposition of Data into Seasonality, Trend and Remainder:
When using raw data
	precip = f(SST)
When using deseasonalised data:
	f.e. remainder precip = f(remainder precip)
	BUT here problem with how to compute stl,
	because stl may use the information of the whole time series that it is computed on.
	1. stl() on all the data:
		 results in the CV fitting data(*1) will contain information from the external validation set
	2. stl() only on the CV fitting data:
		results in training data in each fold will contain information from the test data
		for that fold.
	3. stl() inside each fold same as above in 2.
	4. stl() only on training and testing data seperately in each fold
		stl() on newx use for f(stl(newx)$remainder) = predicted remainder precip
		and then test predicted remainder precip - stl(precip$remainder)
		Last approach would be possible I guess

(*1), CV fitting data: the data that is used for the CV, 
	meaning traintest for fold1,..., traintest for fold5

Precipitation/ drought index:
Compute drought index from precip, summarise precip

Coefficient pre selection:
Reduce number of locations considered by prefiltering with correlation coefficients
	Correlation (precip, SST), only consider locations with large correlations
	Here again problem, how to do correlation analysis? Inside fold or over complete
		dataset? Eventually Information leakage
		If done at lowest level, models are eventually harder to compare

smaller location window:
small or complete 



Models:
LASSO
	plain
	plain + trend + seasonal factor
	plain + trend + seasonal factor + autoregressive timelags
	plain + trend + seasonal factor + autoregressive timelags +/ running means/interactions etc
Fused LASSO
	plain
	other variables dont know how to add yet, season trend etc.

raw or anomalies = 2
precip res = 3
raw or des = 2
drought index or not = 2
coeff pre sel = 2
smaller loc = 2
models = 2 (+feature engineering)

2^6*3 = 192
192 possibilities, but not all might make sense
	for example computing stl on anomalies and compute drought index too, might make
	no sense
precip resolution: depends on performance of mean CAB prediction
deseasonalise is more algorithmic than model dependent -> NO 
drought index no maybe backlog -> NO 
coeff pre selection, too hancrafted, (if then rather smaller window) -> NO 
small window (maybe) -> NO
models (yes) -> YES

In summary:
Deseasonalising (aka using STL), is a bit weird for the CV when used inside the folds
	but when used on the whole data there is information leakage. Meaning we use
	information that from test or validation data that would not be present on training time
	-> Won´t do (only if Niklas gives strong VETO)
Drought index, wont do since its more like a summary on the target and we could do that 
	also after predicting rain, meaning predicting rain is the "stronger" statement.
	-> Won´t do 
Coefficient pre selection, same problem as with stl on the information leakage.
	Also seems a bit to handcrafted, not very elegant.
	-> Won't do

Save to say:
We will do more feature engineering on the LASSO
We will fit the FUSED LASSO
Also we might want to predict the rain with a timegap between last observed SST and Precip
	Meaning from January SST we want to predict f.e rain in May
Depending on how "good" the final models' perfomance is we might increase the number of models
	we fit. Meaning one CV run on each cluster (and computing the mean in each cluster as
	target). Or in the most complex approach compute a CV run on each location in the
	Central Amazon Basin.