Methodische Aspekte und Implementation

Kurz zu den Überlegungen bisher und weitere Möglichkeiten
Evaluierung von Modellen im Allgemeinen
Offene Fragen


welche Methoden/Implementationen kommen deiner 
Meinung in Frage, was für Probleme antizipierst du, etc)

Methodische Aspekte:
lineare Korrelationsanalysen
Als Basis Modell erster Überblick über Zusammenhang
fehlen von linearer Korrelation schließt nichtlineare Korrelation nicht aus
Bisher betrachtet:
Korrelation zwischen "gemittelter Regen im Amazonasbecken" und SST.(je monatl)
Das gleiche für deseasonalised Daten.
Außerdem möglich:
Korrelationen zwischen den einzelnen SST Zeitreihen.
Eventuell Cluster oder Netzwerkanalysen
Autoregressionsanalyse der Regenzeitreihe mit sich selbst.

Modelle:
klassisches Lineares Modell nicht möglich weil n << p
eventuell vorab Regionen im Meer identifizieren und nur diese einschließen

Variable Selection mit Lasso

Lasso mit Vorinformation: Distanz zwischen Meeresregion und Amazonasbecken
Vorabgegclusterte Regionen als Gruppe definieren, nur einen Punkt dieser Gruppe
einschließen oder die ganze Gruppe, vielleicht Gruppe gemittelt als eine Variable einschließen?
Dafür müssten Gruppen aber sehr stabil identifizierbar sein.

Evaluierung der Modelle:
klassische Cross Validation:
bias (approximation) variance (estimation) trade off
minimising AIC also means minimising CV TRUE FOR ALL MODELS
BIC and AIC are same or BIC has fewer terms, assymp for linear models
Last block CV assures indpendence but makes no efficient use of data
Classic CV for time series does not serve indpendence
Results from Paper show that classic CV is no problem for stationary TS
We can use last block independent CV to be sure
	last block, we dont "forecast" on past data
	independent, we dont use train data that could contain info from test
		we have to find out reasonable time lags
		after deseasonalising the longer time lags might have disappeared


quick note:
I would use CV to estimate error for a model approach hence model evaluation
do this for all models and choose best model
best model is refit on all test+train and checked on validate.

Implementation:
LASSO
Choose Validation set, end of time series
choose number of CV blocks
choose ratio of train and test
choose length of prediction gap/horizon
for each lambda
	for each train/test block
		compute lasso model given lambda on train
		compute MSE on test
		report MSE 
	report 5 point summary of MSE's
although I could imagine its more efficient to do it 
for each train/test block
	for each lambda
because we dont need to have all the blocks loaded all the time, but dont optimise too early 

https://topepo.github.io/caret/data-splitting.html, 4.3 Data Splitting for Time Series
	With fixed window, ratio train/test always remains the same
	BUT: When we arrange train/test like this over time, some observations will be more
		often included in train/test than others.
		Namely those that are more "in the middle" of the time frame.
https://rpubs.com/crossxwill/time-series-cv
	When we arrange data like this the train size will vary over time.

	
average MSE over all blocks
we can do this for LASSO and also elastic net.



Offene Fragen/ Methoden/ Outlook:
https://www.amazon.com/gp/product/0387953612?ie=UTF8&tag=prorobjhyn-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0387953612
https://stats.stackexchange.com/questions/139175/aic-versus-cross-validation-in-time-series-the-small-sample-case
"Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure"
Autocorrelation Plots
Variogramms
Correlograms

Is it ok when train and test data overlap?
	Eventual Problem: Data in the middle of window is used more often than
		data on the outside of the window
Is it a problem to use this rolling kind of approach?
	Why could it be? Models are trained with different train/test length
	Use fixed train/test ratio!
Can blocks overlap?
Warum deseasonalised?
Wie lange brauchen erhöhte CO2 Konzentrationen bis sie SST erhöhen?
	maximaler forecast window ergibt sich dadurch evtl?
stability selection
.632 + bootstrap method
	when bootstrapping each obs has prob of 63.2% percent of being included
we cannot do these kind of things because we dont allocate randomly
we could do a lot of rounds of non dependent CV
we can combine all CV possibilities to see how they defer
	classic CV
	blocked CV
	non dependent CV?
