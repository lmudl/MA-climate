---
title: "Problems Summary"
output:
  html_document:
    toc: true
---

# The Data
OPEN: Summarise the data here and where they come from
Also explain autocorrelation and time structure
# General Problem

In our case we have information about monthly sea surface emperature (SST) and monthly precipitation in the Central Amazonas Basin.
From a physics point of view it makes sense that we can make predictions about precipitation based on these SSTs. The question is now how well can we predict precipitation based on the SSTs. Desirably we can not only predict precipitation well, but also identify those regions that give us the most information about future precipitation. So for future predictions it may be enough to only observe these important regions.
In the following we will describe this problem 
from a regression perspective where we start with no covariates, then discuss the single and multiple covariate regression problem. We will see that for our problem a "normal" multiple regression model is not usable at least not with all the data at hand.

# Model without covariates

Conceptually we study the relationship of a covariate/predictor/feature $X$ (SST) and its influence on a response variable $Y$ (Precipitation).
From the data we are studying the relation

$$(Y_1,X_1),...,(Y_n,X_n) \sim F_{X,Y} $$
Where $(Y_i,X_i)$ is a pair of precipitation and SST data for a month $i$. And $F_{X,Y}$ is the
joint distribution.
As stated above our main goal is predicting a value for $y$. Assume now that we could not observe $x$, therefore our best guess for a prediction, was the expected value of $y$,
given by

$$\mathbb{E}(y) = \int yP(Y=y) dy$$

and discrete

$$\mathbb{E}(y) = \sum yP(Y=y)$$


and in case of data
$$\mathbb{E}(y) = 1/n\sum_{i=1}^{n}y_i$$
Where $y_i$ is one of $n$ realizations of the random variable $Y$.
This is true when we use the Mean Squared Error (MSE) to asses the quality of predictions $\hat{y}$.

$$MSE = 1/n\sum_{i=1}^{n}(y_i-\hat{y_i})^2$$
Next we inspect the simple linear regression model.

# Simple linear regression

Now as we know we dont observe our $Y$ (Precipitation) in isolation but also have information from $X$ (SST).
In the same way $\mathbb{E}(y)$ gives us the best $\hat{y}$ for MSE, it can be shown that 
when we also observe $X$, the best predictions are given by the conditional expectation:

$$\mathbb{E}(Y|X) = \int yf(y|x)dy = \int y \frac{f(x,y)}{f(y)} dy$$

and discrete 

$$\mathbb{E}(Y|X) = \sum yf(y|x) = \sum y \frac{f(x,y)}{f(y)}$$
We assume that the regression function
is linear and start with only one covariate
 
$$f(x) = \beta_0 + \beta_1x$$

So we say we can fit a line through the mapped values of X against Y.
Then the model is

$$ y_i = \beta_0 + \beta_1x_i + \epsilon_i$$
$\mathbb{E}(\epsilon_i|x_i) = 0$ and $\mathbb{V}(\epsilon_i|x_i)=\sigma^2$

So given our data $X$ the errors we make (denoted by $\epsilon$) we expect them to be 0.
and that the variance of the errors does not depend on x, after we account for the $X$ in the formula.
The fitted line is then 

$$ \hat{f}(x) = \hat{\beta_0} + \hat{\beta_1}x$$

Predicted values are given by $\hat{y_i} = \hat{f}(x_i)$.
Residuals are 

$$\hat{\epsilon}_i = y_i - \hat{y}_i =  y_i - (\hat{\beta_0} + \hat{\beta_1}x_i) $$

We then would like to find $\hat{\beta_0}$ and
$\hat{\beta_1}$ that miminise the residuals sums of squares (RSS):

$$ RSS = \sum_{i=1}^{n}\hat{\epsilon_i}^2$$

The estimators that minimise the RSS are called the least squares estimates.

$$\hat{\beta_1} = \frac{\sum_{i=1}^{n}(X_i-\bar{X}_n)(Y_i-\bar{Y}_n)}{\sum_{i=1}^{n}(X_i-\bar{X}_n)^2} $$  

$$\hat{\beta_0} = \bar{Y}_n - \hat{\beta_1}\bar{X}_n $$
And the unbiased estimate of $\sigma$^2:

$$\hat{\sigma}^2 = (\frac{1}{n-2})\sum_{i=1}^{n}\hat{\epsilon_i}^2 $$

# Multiple linear regression

We shortly revisit the data from

$$(Y_1, X_1),...,(Y_i,X_i),...,(Y_n,X_n)$$
and 

$$X_i = (X_{i1},...,X_{ik})$$
$X_i$ is a vector of k covariate values (SST values) for the $i^{th}$ observation (the $i^{th}$ month).
As we assume our problem takes the form of a linear model the linear multiple regression model
is

$$Y_i = \sum_{j=1}^{k} \beta_i X_{ij} + \epsilon_i  $$ 
for $i = 1,...,n$ and $\mathbb{E}(\epsilon_i|X_{1i},...,X_{ki}) = 0$.
An intercept can be included by setting $X_{i1}=1,...,n$.
We can write this in matrix notation with
outcomes given by 

$$ Y = \begin{pmatrix} Y_1\\Y_2\\.\\.\\.\\Y_n 
      \end{pmatrix} $$

and the covariates matrix as

$$X = \begin{pmatrix} 
  X_{11} & X_{12} & ... & X_{1k} \\
  X_{21} & X_{22} & ... & X_{2k} \\
  . & . & . & . \\
  . & . & . & . \\
  . & . & . & . \\
  X_{n1} & X_{n2} & ... & X_{nk}
\end{pmatrix}
$$
Each row is an observation (one month of SST values) and the columns are one of k covariates (each column is one gridpoint in the sea).
X is a $(n x k)$ matrix. We also write

$$\beta = \begin{pmatrix}
\beta_1 \\ .\\.\\.\\\beta_k 
\end{pmatrix} \text{ and } 
\epsilon = \begin{pmatrix}
\epsilon_1\\.\\.\\.\\ \epsilon_n
\end{pmatrix}.$$

And so we can write the multiple liear regression formula in matrix notation

$$ Y = X\beta + \epsilon$$

Now if we assume that the $(k x k)$ matrix given by $X^TX$ is invertible, we have:

$$ \hat{\beta} = (X^TX)^{-1}X^TY \\
  \mathbb{V}(\hat{\beta}|X^n) = \sigma^2(X^TX)^{-1} \\
  \hat{\beta} \approx N(\beta, \sigma^2(X^TX)^{-1})
$$

The estimated regression function is given as:

$$\hat{f}(x) = \sum_{j=1}^k \hat{\beta}_j x_j $$
and the unbiased estimator of $\sigma^2$

$$\sigma^2 = (\frac{1}{n-k})\sum_{i=1}^n \hat{\epsilon}_i^2$$
where $\epsilon$ is a vector of residuals $X\hat{\beta}-Y$.
The approximate $1-\alpha$ confidence interval for $\beta_j$ is

$$\hat{\beta_j} \pm z_{\alpha/2}\hat{se}(\hat{\beta_j})$$

and $\hat{se}^2(\hat{\beta_j})$ is the $j^{th}$ diagonal  element of the matrix $\hat{\sigma}^2(X^TX)^{-1}$.

Anyway we have to stop here, because in our case the above estimation equations can not be used
since our data data matrix $X^TX$ is not invertible.
$X$ is $n x p$, and p > n, therefore fat and

$$rank(x) = rank(X^TX) \leq n < p $$
So $X^TX$ does not have full rank and is not invertible.

Comment:
We could still use pseudo inverse to find the best solution.
Bu the solution would have a large condition number meaning
that small changes in the entries of the data can lead to
large changes in the solution, clearly not desirable

# Shrinkage Mathods
Subset selection is interpretable, lower prediction error than full model but it is discrete process therefore high variance and doesnt reduce prediction error. shrinkage models are more continuos and dont suffer so much from high variability.

## Ridge
SEE ESL, p.82 pdf
Ridge shrinks regression coefficients by adding a penalty on their size.

OPEN: Use mathipix here for Ridge formulas.

$\alpha$ is complexity parameter controlling amount of penalty/shrinkage. Coefficients are shrunk towards each other and zero.
In general, in linear regression model, with many correlated 
variables they are poorly determined and suffer from high variance. Size constraint as seen above can help with this problem. Not equivariant under scaling, so standardize before.
Estimate intercept as average and then use centered inputs.
Therefore input matrix has p columns (intercept already dropped).
In matrix notation can be seen that ridge regression solution
adds positive constant to diag in $X^TX$ therefore even originally singular problems become singular even when $X$
has not full rank.
If inputs are orthonormal ridge only rescales least squares estimates.
SKIP: bayesian interpretation, but. ridge estimate is the mode of posterior distribution. distribution is gaussian, therefore also posterior mean.
*Singular value decomposition* perspective: Ridge regression
shrinks according to size of eigenvalues of respective eigenvectors. Along those eigenvectors that explain less variance (smaller eigenvalue) the shrinkage is stronger.
Those directions with higher explanatory power are preserved.
see *eigen decompostion*, eigenvectors are called principal components.
*Implicit assumption*: response will vary with most directions of high variance of inputs. reasonable assumption often but does not need to hold in general.

## Lasso
Shrinkage method like ridge but difference is l1 norm.
No closed expression as in ridge.
Lasso does continous subset selection. If parametrization 
is done according to $t_0$/2 ($t_0$ sum of magnitude of least squares estimates) the least squares are shrunk by about 50% on average.

## Elastic net
Compromise between ridge and lasso. Selects variables 
like lasso, shrinks together coefficients of correlated predictors like ridge, also computationally advantages.



OPEN: Least Angle, Partial Least Squares, grouped lasso,
OPEN: multiple outcomes, may be interesting for the different
  regions.
  


# The LASSO
We want to create a model that has predictive and
explanatory power. Predictive power meaning it can
predict the precipitation in the Central Amazonas Basin, "reasonably well". Explanatory power in the sense of being interpretable, so that we can identify those regions in sea
that give us most information about future precipitation. Our problem setting is high dimensional with n << p. The number of predictors is a lot bigger than the number of actual observations.
This creates issues with a classic linear model since the linear problem is underdefined.
One possible model for the problem at hand is a LASSO regression model.

In general for the linear model:

$$y_i = \beta_0 + \pmb{x}_i^T\pmb{\beta} + \epsilon_i  $$
Where the $Y_i$ refers to the mean precipitation for a given month $i$ and $x_i$ is the vector of sea surface temperature at different locations around the globe.
$\epsilon_i$ is the residual and we wish to estimate the $\beta$Â´s from the data.
As already stated this is not possible with a classic linear model since the number of predictors exceeds the number of observations. We therefore can not estimate a $\beta$ for every grid point in the sea.
From a physical point of view it also seems reasonable that some regions in the ocean have a higher predictive power than others. For example regions that are closer to the Amazonas may have more influence on precipitation in the same month. But regions more far away may have more information on the precipitation half a year in the future.
We therefore would like to use a model that can find the most important regions in the sea for predicting precipitation for some point in the future.
One possible solution for this is a LASSO or elastic net regression model, as implemented in
R by the glmnet package.
This model "automatically" performs model selection, but be aware that because of the time dependencies in our data, normal Cross Validation methods may be unjustified or at least have to be applied with caution.
The glmnet package implements the regression problem in the following manner, solving:

$$\min_{\beta_0, \beta} \frac{1}{N} \sum_{i=1}^N w_il(y_i,\beta_0 + \beta^Tx_i) + \lambda [(1-\alpha)||\beta||_2^2/2 + \alpha||\beta||_1] $$
This is a lasso regression for $\alpha = 1$ and ridge regression for $\alpha = 0$, $\alpha$ controls the overall strength of regularization or penalty.
Intuitively this means we try to find those $\beta$Â´s that minimise the negative log likelihood of our data (this is equal to maximizing the log-likelihood). But at the same time we can not include too many $\beta$ since this will make the second and third term in the formula grow.
As result the algorithm chooses only those predictors that have the most predictive power.
How many predictors are included depends on the strength of regularization given by $\alpha$
*Remark*: Among strongly correlated predictors only one is chosen in the classical lasso model. Ridge regression shrinks the coefficients to zero. 
Elastic net with $\alpha=0.5$ tends to either include or drop the entire group together.
To specifically choose a group of predictors, variations of the lasso or other models have to be considered.

## Remark on Ridge vs Lasso
Both settings work in n>p because eigenvalues are added to
diagonal making the modified cov matrix invertable .

## LASSO general
ESL p.64 (83 in pdf)
Subset selection is interpretable, lower prediction error than full model but it is discrete process therefore high variance and doesnt reduce prediction error. shrinkage models are more continuos and dont suffer so much from high variability
LASSO minimizes penalized residual sum of squares.


# Preselection of Variables/Preconditioning see ESL 18.6 
Before fitting an actual model we can compute crude correlation values between precipitation and each sea grid point.
We can thereby identify regions that are more strongly correlated with the precipitation than others.
We might "preselect" regions and hence possible predictors by using only a certain amount of those strongly correlated sea regions.
One possibility here would be to include the 5% percent of regions that are most correlated with precipitation. Here we can use quantiles as threshold values.
*Question*: Do we choose them symmetrically? (0.25 and 97.5 as cutoff values or does this also depend on the number of values included on each side?)
Another possibility here is to analyse the correlation among the sea regions themselves, group together points as one single region and average the values of these regions. 
This can be seen or be applied as a form clustering to group variables together and start with a smaller number of predictors before applying the regression model itself.
*Remark*: However this preselection is done, it makes sense to include it in the model selection process because this procedure is too a part of the algorithm and needs to be considered.




# Model Selection 
Usually to assess the performance of a model we can use Cross-Validation. We randomly assign observations to test and train sets, the model is then fitted on the train data and used to predict observations in the train set. We repeat this procedure several times and average the errors made. When this procedure is done for different models we can compare the models and choose the one who produces on average the better predictions.
But since we have time dependencies in our data we have to account for this kind of connection in the data and can not use the basic cross validation since it assumes the observations are independent.
Some authors suggest that for stationary time series normal CV can be used, while others argue one should use some CV variations that account for the dependencies.
In a classical time series framework models are often validated by out-of-sample predictions. Given 10 years of data one might fit a model on the first 9 years and predict on the 10th year. While this preserves the natural time order of the data it does not make efficient use of the limited amount of data. 
In a classical CV setting we can train, predict and report errors depending on the number of folds used.
So we want to find a solution that maybe can account for the time order but also makes efficient use of the data.
One possibility here is a form of blocked CV, f.e
train on the first 4 years predict the 5th, train
on year 2 until 5 and predict the 6th,..., train
on year 6 until 9 and precit the 10th.
*Question*: How do we account for the fact that some
observations are used more often than others in this algorithm? F.e year 1 and year 9 are only used once for training while year 5 is used 4 times. Eventually weight according to appearance in train but also in test?

# Timelags
The first correlation analysis also showed that the regions with the strongest correlation change with the prediction horizon. Meaning if we want to predict precipitation 3 months from now, different regions are interesting than we want to predict 1 year from now.
Do we fit different models for different timelags
or do we use the information of different timepoints and regions?

# Graphical LASSO
Network approach to the regularised (grouped) prediction problem.


# Fused LASSO
Group Lasso for structurally grouped features.
Fused Laso structural grouping arises from underlying index
set such as time or for our case location (SL with spars)
See Paper send by Fabi
""


# Open
General assumptions of linear models\
General assumptions LASSO \
Check clean, unified notation\
Definition unbiased\
Q: Intuition behind estimated $\beta_1$, denominator looks like covariance\
Q: do we say when the conditional expectation is 0 and conditional variance is sigma, that they are independent?\
Q: how do we formalise the conditional independence of the $y_i$Â´s given $x$?\
Q:Expected value is minimising mean squared error
but we use sum of sqaured errors, 
Expectation minimises both?\
Q: Least squares fitting is reasoable if trainig observations are independent random draws from population, even if not random still valid, if y_i conditionally independent given input xi.\
Q: Does that mean for example after accounting for confounders the y_i are cond independent?\
*Remark*: RSS makes no assumption about validity of model. If we can predict well, we say model is good enough.
Checkout: Non-negative Garotte
Q: What are the requirements for 1,0? How do you meaure/ which
criteria are used?
Relevant:
https://journals.ametsoc.org/view/journals/clim/aop/jcliD200079/jcliD200079.xml


# Resources
All of statistics\
ESL\
https://www.le.ac.uk/users/dsgp1/COURSES/THIRDMET/MYLECTURES/1REGRESS.pdf
https://www.math.arizona.edu/~tgk/464_07/cond_exp.pdf