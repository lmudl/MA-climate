---
title: "Fit Linear Models"
author: "Dario Lepke"
date: "6 8 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("./../../"))
```
```{r}
set.seed(1234)
library(raster)
library(ggplot2)
library(dplyr)
library(ncdf4)
```



```{r}
precip <- readRDS("data/processed/decomposed_precip_data.rds")
dim(precip)
sst <- readRDS("data/processed/decomposed_sst_data.rds")
dim(sst)
```
```{r}
#plot(density(precip))
```
```{r}
precip_means <- apply(precip, 2, mean)
#plot(density(precip_means))
```


```{r}
precip_scaled <- apply(precip, 2, scale)
#plot(density(precip_scaled))
```
```{r}
precip_means_after_scaling <- apply(precip_scaled, 2, mean)
#plot(density(precip_means_after_scaling))
```

also compute means of precip data easy
```{r}
# dim(precip)
# precip_means <- apply(precip, 2, mean)
# dim(precip_means)
# str(precip_means)
# plot(precip_means)
# plot_ts(precip_means)
```

```{r}
library(glmnet)
df <- data.frame(cbind(precip_means, t(sst)))
varmtx <- model.matrix(precip_means~.-1, data = df)
dim(varmtx)
response <- df$precip_means
```

Fit full ridge and lasso, for cross validation we might need to
use other methods.

```{r}
ridge <- glmnet(x = scale(varmtx), y = response, alpha = 0)
plot(ridge)
```

```{r}
lasso <-  glmnet(x = scale(varmtx), y = response, alpha = 1)
plot(lasso)
```
```{r}
pl <- print(lasso)
pl[1:50,]
```
```{r}
coef(lasso, s=40)
rownames(coef(lasso, s = 40))[coef(lasso, s = 40)[,1]!=0]
```
Plot these variables
SO we have V878 and V10830
get from these lines of the matrix the respective lotlan

```{r}
ersst <- brick("data/interim/sst/ersst_setreftime.nc",
             varname= "ssta")
a <- xyFromCell(ersst, 878)
b <- xyFromCell(ersst, 10830)
plot_cell(a)
plot_cell(b)
```

```{r}
# m <- as.matrix(ersst[[1:2]])
# ind <- apply(m, 1, function(x) all(!is.na(x)))
# length(ind)
# 
# m <- m[!ind, ]
```




get lets stay all variables so that deviance is over 90

```{r}
rownames(coef(lasso, s = 12))[coef(lasso, s = 12)[,1]!=0]
```
```{r}
# cut intercept out
# save list of var names
rn <- rownames(coef(lasso, s = 12))[coef(lasso, s = 12)[,1]!=0][-1]
# for each rowname cut the V out
cut_v <- function(rownames) {
  for (i in 1:length(rownames)) {
    rownames[i] <- strsplit(rownames[i], "V")[[1]][2]
  }
  return(rownames)
}

rn <- cut_v(rn)

# use var names as literal programming in xyfromcell
# eval(parse(text="2+2"))
rownames_to_xy <- function(rownames, raster) {
  l <- length(rownames)
  m <- matrix(NA, nrow = l, ncol = 2,
              dimnames = list(NULL,c("x","y")))
  for (i in 1:l) {
    ex <- parse(text = rownames[i])
    ev <- eval(ex)-1
    xy <- xyFromCell(raster, ev)
    m[i,] <- xy
  }
  return(m)
}

test_m <- rownames_to_xy(rn, ersst)

# use new function similar to plot cell but for multiple cells
plot_multiple_cells <- function(multiple_cells) {
  # Using GGPLOT, plot the Base World Map
  mp <- NULL
  mapWorld <- borders("world", colour="gray50", fill="gray50") # create a layer of borders
  mp <- ggplot() +   mapWorld
  
  #vNow Layer the cities on top
  mp <- mp + geom_point(data=as.data.frame(multiple_cells), aes(x=x, y=y), ,color="blue", size=1)
  mp
}

plot_multiple_cells(test_m)
```
```{r}
?coordinates
```



```{r}
# open: normal lm to
ridge_cv <-  cv.glmnet(x = scale(varmtx), y = response, alpha = 0)
lasso_cv <-  cv.glmnet(x = scale(varmtx), y = response, alpha = 1)
```
```{r}
plot(ridge)
```
```{r}
plot(lasso)
```

```{r}
(ridge_min <- min(ridge$cvm))
(lasso_min <- min(lasso$cvm))
```

```{r}
df_scaled <- data.frame(cbind(precip_means_after_scaling, t(sst)))
varmtx_scaled <- model.matrix(precip_means_after_scaling~.-1, data = df_scaled)
dim(varmtx_scaled)
response <- df_scaled$precip_means_after_scaling
# normal lm to do
ridge_as <-  cv.glmnet(x = scale(varmtx_scaled), y = response, alpha = 0)
lasso_as <-  cv.glmnet(x = scale(varmtx_scaled), y = response, alpha = 1)
```


```{r}
(ridge_min <- min(ridge$cvm))
(lasso_min <- min(lasso$cvm))
```

```{r}
plot(ridge)
```
```{r}
plot(lasso)
```
```

```{r}

```


#######################
```{r}
dim(sst)
?cor()
cor(precip_means, sst[1,])
t_corr <- c()
for (i in 1:nrow(sst)) {
  t_corr[i] <- cor(precip_means, sst[i,])
}
max(t_corr)

d <- function(vec, drops) {
  object <- object[]
}

drop_lastn <- function(obj, lastn) {
  lo <- ncol(obj)
  startdrop <- lo - lastn
  obj <- obj[,-c(startdrop:lo)]
  return(obj)
}

compute_corr <- function(target, predictors, delay=0) {
  corr_vec <- c()
  if(delay!=0) {
    target <- target[-c(1:delay)]
    ncol_pred <- ncol(predictors)
    startdrop <- ncol_pred-delay+1
    predictors <- predictors[,-c(startdrop:ncol_pred)]
  }
  for (i in 1:nrow(predictors)) {
    corr_vec[i] <- cor(target, predictors[i,])
  }
  return(corr_vec)
}

nolag <- compute_corr(precip_means, sst,0)
onelag <- compute_corr(precip_means, sst, 1)
twolag <- compute_corr()

corr_matr <- matrix(nrow = 12, ncol = nrow(sst), NA)
for (i in 1:12) {
  corr_matr[i,] <- compute_corr(precip_means, sst, i)
}

dim(corr_matr)
which(corr_matr == max(corr_matr), arr.ind = TRUE)
dim(sst)
df <- data.frame(cbind(precip_means, t(sst)))
ggplot(data = df, aes(y = precip_means, x = sst[10830,])) + geom_point()

```


```{r}
# check size of regression matrix
# 432 times precip, 16020 predictors
# 432 times 16020
ttm <- matrix(1.002, nrow = 432, ncol = 16020)
ttdf <- as.data.frame(ttm)
object.size(ttm)
```

```{r}
object.size(ttdf)
```
```{r}
# trying to estimate runtim for whole precip or sst data on this laptop
library(rbenchmark)
benchmark("27layers" = {
  decompose_brick(brick = precip, nlayers = 27, ncells = 10)
},
"54layers" = {
  decompose_brick(brick = precip, nlayers = 54, ncells = 10)
},
"108layers" = {
  decompose_brick(brick = precip, nlayers = 108, ncells = 10)
},
replications = 1,
columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))

```
```{r}
benchmark("27layers" = {
  decompose_brick(brick = precip, nlayers = 27, ncells = 50)
},
"54layers" = {
  decompose_brick(brick = precip, nlayers = 54, ncells = 50)
},
"108layers" = {
  decompose_brick(brick = precip, nlayers = 108, ncells = 50)
},
replications = 1,
columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))
```

```{r}
comp_fac <- function(brick_object, nc, nl) {
  return(ncell(brick_object)/nc)+(nlayers(brick_object)/nl)
}

comp_fac(precip, 50, 108)
1224*0.87/60/60 #probably will take 30m mins

```

We can use Raster Time Series!!!
rts()
https://cran.r-project.org/web/packages/rts/rts.pdf
https://datacarpentry.org/r-raster-vector-geospatial/12-time-series-raster/

https://rdrr.io/github/ffilipponi/rtsa/man/rtsa.stl.html

```{r}
library(rts)
slice <- precip[[1:38]]
precip

slice <- rts(slice)

netcdf_precip <- nc_open("data/interim/drought/chirps_setreftime.nc")
t <- ncvar_get(netcdf_precip, "time")
netcdf_precip

?convertDateNcdf2R

install.packages("ncdf4.helpers")
?ncdf4.helpers::nc.get.time.series()

t <- ncdf4.helpers::nc.get.time.series(netcdf_precip)
t <- as.Date.POSIXct(t)
rts_test <- rts(precip, t)
rtsa::rtsa.stl(rts_test)


```
