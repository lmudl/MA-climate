---
title: "Detrend and Deseasonalise Time Series"
author: "Dario Lepke"
date: "11.08.21"
output: 
  html_document:
    toc: true
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = normalizePath("./../../"))
knitr::opts_knit$set()
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r, include = FALSE}
work_dir <- getwd()
# html file will be written to Rmd dir
#rmarkdown::render("code/Rmarkdown/Deasonalise and Detrend.Rmd",
                  # chunks will be evaluated with this working dir
 #                 knit_root_dir = work_dir, 
                  # md file to be converted by pandoc will be in working dir
  #                intermediates_dir = work_dir)
```


```{r}
set.seed(1234)
library(raster)
library(ggplot2)
library(dplyr)
library(ncdf4)
#getwd()
```
### Helper functions {.tabset}

#### Hide

#### Show
```{r helper functions}
# as.matrix, each col is one layer, meaning stl() over
# the rows is stl() on each grid cell time series
brick_to_matrix <- function(brick_object, nlayers) {
  # as.matrix comes here from raster, so raster::as.matrix()
  m <- as.matrix(brick_object[[1:nlayers]])
  ind <- apply(m, 1, function(x) all(is.na(x)))
  m <- m[!ind, ]
  if(any(is.na(m))) stop("Matrix still contains NA")
  return(m)
}

# in: brick object
# out: random gridcell that is not on land
#      x is longitude, y is latitude

get_random_cell <- function(brick) {
  # get one layer as a raster object
  r <- raster(brick, layer = 1)
  l <- length(r)
  vals <- getValues(r)
  not_na <- which(!is.na(vals))
  rnd_cell <- sample(not_na, 1)
  lonlat <- xyFromCell(r, rnd_cell)
  return(lonlat)
}

# in: lonlat info of sea grid point
# out: a ggplot of point on map
plot_cell <- function(cell) {
  # Using GGPLOT, plot the Base World Map
  mp <- NULL
  mapWorld <- borders("world", colour="gray50", fill="gray50") # create a layer of borders
  mp <- ggplot() + mapWorld
  
  #vNow Layer the cities on top
  mp <- mp + geom_point(aes(x=cell[1], y=cell[2]), ,color="blue", size=3)
  mp
}

# in: dataset and respective cellpoint
# out: ggplot of time series data for cellpoint
get_ts <- function(data,cell) {
  vals <- c(extract(data,cell))
  return(vals)
}

plot_ts <- function(ts) {
  df <- as.data.frame(ts)
  df$month <- 1:length(ts)
  ggplot(df, aes(x=month, y=ts)) + geom_point() +
    geom_line() + geom_smooth()
  
}

plot_stl <- function(values) {
  time_series <- ts(values, frequency = 12)
  time_series %>% stl(t.window = 12, s.window = "periodic", robust = TRUE) %>% forecast::autoplot()
}
```


## Quick review of research question

- Former research suggests it is possible to predict rainfall/drought in the Central Amazonas Basin (CAB) from preceding sea surface temperatures (SSTs)
alone
- In a first step we want to fit regularized linear models (f.e LASSO) to the 
data. (Predictors: Each longitude/latitude grid point is one, Target: Precipitation in the CAB). To identify important predictive locations in the sea.

## What is done so far
- Preprocessing the raw data (choose years that are covered by both datasets,
change orientation of grid cells, set reference time of the timestamps).
- Applying seasonal trend composition using loess (STL) on each single time series in the data set (see below)

##  Datasets, quick overview {.tabset}

### Sea Surface Temperature

#### Load data
```{r}
sst <- brick("./data/interim/sst/ersst_setreftime.nc",
             varname= "sst")
sst
```
The data is organised on a 2x2 latlon grid with 432 months of observations (=layers).

#### Plot density
```{r}
vals <- values(sst)
range(vals, na.rm = TRUE)
```

```{r, eval = FALSE}
df <- as.data.frame(c(vals))
dense_sst <- ggplot(data = df, aes(x = vals)) + geom_density()
dense_sst
ggsave("plots/dense_sst.jpg")
```

```{r, echo = FALSE}
knitr::include_graphics("plots/dense_sst.jpg")
```

The maximum and minimal values for the sst data seem to be pretty rare,
while most values lie in the range of +- 2.5.
Are these values too extreme to be reasonable?

#### Plot random grid cell

We will now extract one time series of values for random grid point that is not NA (The SST data contains NA on continental grid points).

```{r}
sst_cell <- get_random_cell(sst)
```

And plot that point on the map.

```{r, eval = FALSE}
plot_sst_cell <- plot_cell(sst_cell)
plot_sst_cell
ggsave("plot_sst_cell.jpg")
```
```{r, echo = FALSE}
knitr::include_graphics("plots/plot_sst_cell.jpg")
```

We now plot the respective time series of that point.

```{r}
sst_ts <- get_ts(sst, sst_cell)
```

```{r, echo = FALSE}
# sst_ts_plot <- plot_ts(sst_ts)
# ggsave("sst_ts_plot.jpg")
knitr::include_graphics("./plots/sst_ts_plot.jpg")
```
As we would expect for temperature data there is a seasonal effect in it.
Also it seems there is a low upward trend.

### Precipitation

We proceed with the same steps as before now for the precipitation data.

#### Load data

```{r}
precip <- brick("data/interim/drought/chirps_setreftime.nc", varname = "precip")
#mean_test2 <- calc(test2, fun=mean, na.rm=TRUE)
#plot(mean_test2)
precip
```
```{r}
vals <- values(precip)
length(vals) == 61200*432
range(vals, na.rm = TRUE)
```

#### Plot density

```{r, eval = FALSE}
df <- as.data.frame(c(vals))
ggplot(data = df, aes(x = vals)) + geom_density()
ggsave("density_precip.jpg")
```
```{r, echo = FALSE}
knitr::include_graphics("plots/density_precip.jpg")
```

Here also we have some pretty rare, high values. Is this reasonable or should
we exclude them?


#### Plot random grid cell
```{r}
cell_precip <- get_random_cell(precip)
```

Plot the point on the map
```{r, eval = FALSE}
plot_cell(cell_precip)
ggsave("cellplot_precip.jpg")
```
```{r, echo = FALSE}
knitr::include_graphics("./plots/cellplot_precip.jpg")
```


Plot the respective time series
```{r, eval = FALSE}
precip_ts <- get_ts(precip, cell_precip)
plot_ts(precip_ts)
ggsave("precip_ts.jpg")
```

```{r, echo = FALSE}
knitr::include_graphics("plots/precip_ts.jpg")
```


Here again we can recognize a seasonal effect, as well as a small downward trend.


## STL: Seasonal Trend Decomposition using loess

We decompose time series now using the stl function.
The stl algorithm uses local polynomial regression to fit a smoothing line to the data, while deconstructing it into three parts.
$$Monthly \textit{ } Temperatures = Seasonal + Trend + Remainder$$
The estimates for the seasonal effect and trend are computed using local
polynomial regression. To give an estimation $\hat{y_i}$ for a given $x_i$,
we take into account the values of the neighbouring $x_j$, while assigning more
weight to points that are closer to $x_i$. Then a polynomial regression model is fitted to create an estimate for $\hat{y_i}$ at $x_i$. Additionally robustness weights are used that give more weight to estimations $\hat{y_i}$ that are closer to their "original" $y_i$, hence reducing the influence of outliers.
The algorithm estimates the seasonal and trend component iteratively while also using a low-pass filter.

## STL on the data {.tabset}

### Sea Surface Temperature
```{r, eval = FALSE, include = FALSE}
stl_sst <- plot_stl(sst_ts)
ggsave("stl_sst.jpg")
```
```{r, eval = FALSE, include = FALSE}
knitr::include_graphics("plots/stl_sst.jpg")
```

We now compute monthly means for all grid points and deconstruct
these global sea temperatures with the stl() function.

```{r, eval = FALSE}
sst_m <- brick_to_matrix(sst, nlayers = nlayers(sst))
sst_means <- apply(sst_m, 2, mean)
sst_means_plot <- plot_stl(sst_means)
ggsave("sst_means_plot.jpg")
sst_means_plot
```
```{r, echo = FALSE}
knitr::include_graphics("plots/sst_means_plot.jpg")
```

The bars on the side show the relative size of the y-axis to the first panel ("data"). Meaning if a bar is large the graph is "zoomed in". We can see that for the "seasonal" panel, with a y-axis approximately 10 times smaller than the "data" y-axis. The small values indicate that for this time series, the seasonal effect is not as large as for example the trend component (panel two).
We also see that we have quite large values in the remainder panel. Values that can not be denoted to either the trend or seasonal component.


### Precipitation

```{r, eval = FALSE,include=FALSE}
plot_stl(precip_ts)
ggsave("stl_precip.jpg")
```
```{r, eval = FALSE, include =FALSE}
knitr::include_graphics("plots/stl_precip.jpg")
#Here for this specific grid point of the precipitation data we have more of a #seasonal effect, than trend effect.
```

As for SST we compute the mean of all grid points for each month 
and decompose the resulting time series via stl().

```{r, eval = FALSE}
precip_m <- brick_to_matrix(precip, nlayers = nlayers(precip))
precip_means <- apply(precip_m, 2, mean)
plot_stl(precip_means)
ggsave("stl_precip_means.jpg")
```

```{r}
knitr::include_graphics("plots/stl_precip_means.jpg")
```

For the mean precipitation in the CAB we see that we have a strong seasonal effect, also with a lot of large remainders.

## Decompose both datasets

We now apply the stl decomposition on the time series of each grid cell,
on both data sets. We then extract only the remainder parts of the stl decomposition so we can use them as the input for a LASSO model.

```{r, include=FALSE, eval=FALSE}
# as.matrix, each col is one layer, meaning stl() over
# the rows is stl() on each grid cell time series
brick_to_matrix <- function(brick_object, nlayers) {
  # as.matrix comes here from raster, so raster::as.matrix()
  m <- as.matrix(brick_object[[1:nlayers]])
  ind <- apply(m, 1, function(x) all(is.na(x)))
  m <- m[!ind, ]
  if(any(is.na(m))) stop("Matrix still contains NA")
  return(m)
}
```


```{r}
decompose_matrix <- function(matrix_with_ts, ncells) {
  # use min here, since if data contained NA (like sst),
  # we removed the NAs (which were grid points on land)
  # so ncell may be bigger than the actual number of 
  # nonempty rows
  ncells <- min(ncells, nrow(matrix_with_ts))
  out <- apply(matrix_with_ts[1:ncells, ], 1, function(x) if(!any(is.na(x))) stl(ts(x, frequency = 13), "periodic"))
}

# List of length "ncells", each has list of 8, we are interested in the dec$time.series[,"remainder"]

# take the decomposed time series and fill an empty matrix with the time series
# of "remainder" values for each time series respectively.
# nrow depends on the number of decomposed cells
# ncol depends on how many months were decomposed for each cell
get_remainder <- function(list_of_decomposed_ts) {
  l <- length(list_of_decomposed_ts)
  out <- matrix(NA, nrow = l,
                ncol = nrow(list_of_decomposed_ts[[1]]$time.series))
  for (i in 1:l) {
    out[i,] <- c(list_of_decomposed_ts[[i]]$time.series[,"remainder"])
  }
  return(out)
}

# use all functions together to decompose a brick object in one step
decompose_brick <- function(brick, nlayers, ncells) {
  ts_matrix <- brick_to_matrix(brick_object = brick, nlayers = nlayers)
  dec_list <- decompose_matrix(matrix_with_ts = ts_matrix, ncells = ncells)
  remainder <- get_remainder(list_of_decomposed_ts = dec_list)
  return(remainder)
}

```


```{r}
#library(tictoc)
#tic()
#decomposed_precip <- decompose_brick(brick = precip, nlayers = nlayers(precip), ncells = ncell(precip))
#toc() #took 90s
#saveRDS(decomposed_precip, file = "./../../data/processed/decomposed_precip_data.rds")
precip <- readRDS("data/processed/decomposed_precip_data.rds")
dim(precip)
```
```{r}
#tic()
#decomposed_sst <- decompose_brick(brick = sst, nlayers = nlayers(sst), ncells = ncell(sst))
#saveRDS(decomposed_sst, file = "data/processed/decomposed_sst_data.rds")
#toc()
#sst <- readRDS("data/processed/decomposed_sst_data.rds")
sst <- readRDS("data/processed/decomposed_sst_data.rds")
dim(sst)
```

## Next steps

- Use remainder from stl decomposition as input for LASSO model.
- Find a Cross Validation method for the time series data
- Visualise results

```{r, echo = FALSE}
knitr::knit_exit()
```


```{r}
# check size of regression matrix
# 432 times precip, 16020 predictors
# 432 times 16020
ttm <- matrix(1.002, nrow = 432, ncol = 16020)
ttdf <- as.data.frame(ttm)
object.size(ttm)
```

```{r}
object.size(ttdf)
```
```{r}
# trying to estimate runtim for whole precip or sst data on this laptop
# library(rbenchmark)
# benchmark("27layers" = {
#   decompose_brick(brick = precip, nlayers = 27, ncells = 10)
# },
# "54layers" = {
#   decompose_brick(brick = precip, nlayers = 54, ncells = 10)
# },
# "108layers" = {
#   decompose_brick(brick = precip, nlayers = 108, ncells = 10)
# },
# replications = 1,
# columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))

```
```{r}
# benchmark("27layers" = {
#   decompose_brick(brick = precip, nlayers = 27, ncells = 50)
# },
# "54layers" = {
#   decompose_brick(brick = precip, nlayers = 54, ncells = 50)
# },
# "108layers" = {
#   decompose_brick(brick = precip, nlayers = 108, ncells = 50)
# },
# replications = 1,
# columns = c("test", "replications", "elapsed", "relative", "user.self", "sys.self"))
```

```{r}
comp_fac <- function(brick_object, nc, nl) {
  return(ncell(brick_object)/nc)+(nlayers(brick_object)/nl)
}

comp_fac(precip, 50, 108)
1224*0.87/60/60 #probably will take 30m mins

```

We can use Raster Time Series!!!
rts()
https://cran.r-project.org/web/packages/rts/rts.pdf
https://datacarpentry.org/r-raster-vector-geospatial/12-time-series-raster/

https://rdrr.io/github/ffilipponi/rtsa/man/rtsa.stl.html

```{r}
# library(rts)
# slice <- precip[[1:38]]
# precip
# 
# slice <- rts(slice)
# 
# netcdf_precip <- nc_open("data/interim/drought/chirps_setreftime.nc")
# t <- ncvar_get(netcdf_precip, "time")
# netcdf_precip
# 
# ?convertDateNcdf2R
# 
# install.packages("ncdf4.helpers")
# ?ncdf4.helpers::nc.get.time.series()
# 
# t <- ncdf4.helpers::nc.get.time.series(netcdf_precip)
# t <- as.Date.POSIXct(t)
# rts_test <- rts(precip, t)
# rtsa::rtsa.stl(rts_test)


```


