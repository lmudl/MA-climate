## Model evaluation

```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, message = FALSE, out.width = '50%', fig.align = 'center')
options(knitr.duplicate.label = "allow")
```

In general we use the same evaluation methods for the fused lasso
that we used for the lasso models.
We define 5 folds with train and test, search for an optimal regularization
value ($\lambda_{\min}$). Then we fit the model on the complete train and test
data with $\lambda_{\min}$ and report the MSE on the evaluation set.
But the difference for the lasso and the fused lasso is that we can
define a $\lambda$ vector that we want to search for solutions for the
lasso *before* starting the CV, in the fused lasso this is not possible.
In the *fusedlasso* function we can define the number of steps 
the model should take and a $\lambda$ that defines the end of
the path (we will call it $\lambda_{end}$ here). The model terminates therefore if either the maximum number 
of steps or the defined minimum $\lambda_{end}$ is reached (default is 0).
This means that the first $\lambda_{start}$ for the fused lasso is different in 
each fold and when the maximum number of steps is reached before
the path reached $\lambda_{end}$ the last evaluated (or found) $\lambda_{last}$.
Is not guaranteed to be the same across fold.
So for the fused lasso we can not define the regularization values to evaluate,
but must rather inspect the results for each fold.
We solve this here rather pragmatically.
We inspect MSE lines across the solution path for each fold.
For the overlapping region we define the mean of all evaluated points
and choose $\lambda_min$ as the amount of regularization that minimizes
the MSE for common area of the solution path.
The $\lambda$ values will not exactly be the same for the folds,
so we have to interpolate the gaps between the actual $\lambda$, to create
a common range we can compute the mean on.
The interpolation is done via local polynomial regression fitting
using the *loess* function in R with a span of 0.05 and degree 2 (@cleveland1992local).

## Graph structure

```{r graph-plot2, fig.cap='Graph of the SST and land areas used in fused lasso'}
library(igraph)
k <- readRDS("../data/processed/graph_sst.rds")
cl <- clusters(k)
#png(filename = "graph_plot.png")
#jpeg("graph.jpg")
plot(k, vertex.label = NA, vertex.size=0.00001,
          edge.width= 0.0001,
          vertex.color=cl$membership*10,
     mark.expand = 15)
#knitr::include_graphics("../figures/graph.jpg")
#dev.off()
#knitr::include_graphics("../figures/graph.jpg")

```
```{r}
#knitr::include_graphics("../figures/graph.jpg")
```

The fused lasso function allows the user to specify a penalty matrix or a graph
alongside the input matrix $X$.
If a graph is given, the penalty matrix is computed as the incidence matrix
from the graph. We will describe shortly how we defined the graph.
The SST data is defined on a $89 \times 180$ 2-dimensional grid.
We define a $89 \times 180$ lattice graph, identify those nodes 
that correspond to land areas and delete those nodes.
Because of its resolution and the inclusion of large lakes in the
SST data, this leads to cluster structures in the graph (see \@ref(fig:graph-plot2))
There are 7 clusters present in this graph colored according to their membership.
The smallest cluster is the White Sea in Russia.
The largest cluster are the connected SST nodes after deleting the land nodes.
This separated the Mediterranean and the Red Sea and actually disconnected North and 
South america. The large lake structures are clusters naturally.
Some other structures like the Persian Gulf stay connected with the other SST regions
but due to the original lattice of the graph, the only have few edges.
Since the fused lasso penalizes only the difference of connected nodes,
nodes with fewer edges and hence nodes to be compared to, 
are less penalized in this graph.
We fitted the fused lasso on graph \@ref(fig:graph-plot2) and a graph that has these smaller clusters removed.
What we found is that, deleting the smaller clusters improved
the MSE compared to the fused lasso including the clusters. But still
we can observe that nodes with fewer edges obtain larger coefficient values
in general.
Following we will summarize these findings.



```{r graph-plot, eval = FALSE,fig.cap='Graph of the SST and land areas used in fused lasso, colored vertices indicate sub-graphs'}
knitr::include_graphics("../figures/graph_plot.png")
```

## Results

Computing the fused lasso solution path is computationally very expensive.
We therefore had to choose strategically which model variations we will
investigate.
From the lasso we found that apart from standardizing, neither de-seasonalizing,
nor differentiating the coefficients improved predictive performance on the validation set.
We therefore started with the fused lasso on the original data (without standardization).
As we described earlier, the relation between graph structure, subgraphs (i.e cluster) and its influence on the results is not trivial.
After fitting the fused lasso on the original data and graph, we also fitted
the fused lasso with original data on the graph without clusters.
Comparing these models already showed us a hint that removing the clusters
can improve the predictions, but after all it's only a minor solution, because
differences in connectives in the graph is still existing.
For the sake of completeness we also tried standardizing the data for
the normal and the graph without clusters, none of these two approaches was 
better than the "plain" fused lasso version without clusters.
Eventually we inspected the influence of sparsity on the results, with
values $\gamma = 0.1$, which did not improve predictions on the validation set.
The predictions seemed to underfit and therefore we choose lower regularization
parameter $\gamma = 0.05$, again without improvement.
We therefore only present the results of the best model and refer
for the other models to the supplementary material.



### Fused lasso without sub-graphs

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```


```{r}
library(patchwork)
library(ggpubr)
library(raster)
library(glmnet)
library(Hmisc)
source("../code/R/helper-functions.R")
```

```{r}
path_to_model_folder <- "../results/CV-fused/noclust-large-fused-5k/"
```

```{r}
# err_mat <- readRDS(paste0(path_to_model_folder, "/err-mat.rds"))
#min(apply(err_mat, 1, mean))
#lambdas <- readRDS(paste0(path_to_model_folder, "/lambda-vec.rds"))
```


The error lines in the different folds differ in their trajectories as well as in their starting points (Figure 6.2). Note that we cut off fold 3 for better readability of the plot
(the MSE reaches until 3000). The black line indicates the mean, computed for the area
that is covered by all error lines (after interpolation).

```{r err-line-fused-noclust, out.width='45%', fig.cap="Error lines in the fused lasso forward validation. Each line represents one fold, the black line the mean for the common interval of the smoothed error lines. The red dashed-line shows the minimum of the mean."}

best_l_res <- readRDS(paste0(path_to_model_folder, "best-lambda-res.rds"))
# log(best_l_res$lambda_min)
#best_l_res$err_plot + ylim(c(0,50000)) + xlim(3,6)
best_l_res$err_plot$labels$colour <- "Fold"
#best_l_res$err_plot
p <- best_l_res$err_plot + geom_vline(xintercept=log(best_l_res$lambda_min),
                        linetype="dashed",
                color = "red", size=0.5)
p
# p2 <- p + ylim(0,2000)
# ggsave(paste0(path_to_model_folder,"best-lambda-res.png"),
#        p2)
```


```{r}
pred_plot_1 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-1.rds"))
pred_plot_1 <- pred_plot_1 + ylab("Precipitation fold 1")
mse_1 <- get_mse_from_pred_plot(pred_plot_1)

pred_plot_2 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-2.rds"))
pred_plot_2 <- pred_plot_2 + ylab("Precipitation fold 2")
mse_2 <- get_mse_from_pred_plot(pred_plot_2)

pred_plot_3 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-3.rds"))
pred_plot_3 <- pred_plot_3 + ylab("Precipitation fold 3")
mse_3 <- get_mse_from_pred_plot(pred_plot_3)

pred_plot_4 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-4.rds"))
pred_plot_4 <- pred_plot_4 + ylab("Precipitation fold 4")
mse_4 <- get_mse_from_pred_plot(pred_plot_4)

pred_plot_5 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-5.rds"))
pred_plot_5 <- pred_plot_5 + ylab("Precipitation fold 5")
mse_5 <- get_mse_from_pred_plot(pred_plot_5)

#pred_plot_list <- list(pred_plot_1,pred_plot_2,pred_plot_3,pred_plot_4,pred_plot_5)
```
```{r pred-fold-fused-noclust,fig.cap="Precipitation prediction and target values in the test set in each fold. Predictions in red and target values in black."}
pred_plot_1 + pred_plot_2 + pred_plot_3 + pred_plot_4 +
  pred_plot_5
```
The predictions inside the folds are very similar to lasso without fusion, the same holds for the predictions from
the full model, but the MSE improves here.

```{r pred-plot-full-fused-noclust, fig.cap="Precipitation prediction and target values in the validation set. Predictions in red and target values in black. The model was fitted on the full CV data with the lambda value that minimised the average MSE"}
full_preds <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-full.rds"))
mse_full <- get_mse_from_pred_plot(full_preds)
full_preds
#mse_full
```
On first sight the predictions don't change a lot compared to the lasso model,
but the fused lasso here can predict the evaluation data best so far. (MSE is `r round(mse_full,2)`). 
We can see that the low values at the end of season 2 and 5 (around month 1363 and 1400),
are better predicted than in the standard lasso.
Also the fused lasso predicts the seasonality amplitudes slightly better.
The resulting coefficient plot is given in Figure \@ref(fig:coef-plot-full-fused-noclust).

```{r coef-plot-full-fused-noclust, fig.cap="Coefficient plot of the full fused lasso model trained on the complete forward validation data.Sub-graphs were removed in this model", out.width='50%'}
coef_plot <- readRDS(paste0(path_to_model_folder,
                      "coef-plots/coef-plot-full.rds"))
coef_plot$layers[[2]]$aes_params$size <- 0.75
coef_plot
#coef_plot$labels$colour <- "Coefficients"
#coef_plot$data
#coef_plot_now <- coef_plot
#ggsave(paste0(path_to_model_folder, "coef-plots/coef-plot-full.png"), plot = coef_plot_now)

# knitr::include_graphics(paste0(path_to_model_folder, "coef-plots/coef-plot-full.png"))
```

The coefficent plots for the fused lasso show now the regions that the fused lasso
selected (Figure \@ref(fig:coef-plot-full-fused-noclust). 
As expected the fused lasso assign the same coefficients for areas rather
instead just for single points. 
We can see from
the legend that the coefficient values are skewed, note the large negative 
coefficients in the Baltic sea.
Due to the resulting coloring some regions are difficult to spot, for example
the region before the south american coast of Chile and Peru.
Coefficients with fewer edges, hence smaller regions are given higher values,
as we already described earlier.
We assume that in a fully connected graph, with weights according to distances
these values would not be as large (relatively to other coefficient values) because they too would be more penalized than for the graph we created from the lattice structure.
But regions that have many nodes will still be influential because
their values summarize when used for predictions.
When comparing to the coefficient plot from the lasso,
we see that some areas are included in both models,
for example the positive coefficients around the area Gulf of Guinea,
as well as the negative values around Chile and Peru. In the fused lasso
plot the latter values are smaller in relation to the large negative values 
in the Baltic sea. 


## Summary 
After fitting different possible fused lasso models, we find that
the using the graph without separated clusters on the standard SST data
without regularization worked best.
Regularization did not improve the performance on the validation set.
Our interpretation of the findings is that on one hand the fused lasso
chooses coefficients that are also apparent in the lasso results.
The lasso chooses rather single points and discards the rest of a homogeneous area.
The fused lasso chooses the whole area and distributes the coefficient value
evenly. But the fused lasso does so by penalizing the differences of neighboring
coefficients. How strongly these are penalized depends on their distance that
is defined by a graph (but could also be defined by a penalty matrix directly).
We chose to represent the SST structure as lattice and removed nodes that 
correspond to land.
Locations that contain mainly land and a relatively small area of water,
may be represented as land area as a whole (the opposite is true when
the region contains mainly SST.).
When there is only a small connection between sea areas the connecting region
is therefore represented as land and the corresponding node is deleted
when the graph is created. This creates sub-graphs as we discussed earlier
and showed in Figure \@ref(fig:graph-plot2).
This gives the impression that the sub-graphs are not connected at all to the larger SST
cluster. The Caspian Sea for example occurred as separated cluster, it could have also been
a cluster in the Arctic, the fused lasso has no information anymore about its location
if its a sub-graph. From a physical perspective this makes of course little sense.
One possible solution for this might be to define fully connected graph with
weights according to geographical distance.
If the square root of weights are used in defining the penalty matrix,
$D^TD$ is still the Laplacian and could be used to fit the fused lasso model.
One caveat here is that the runtime of the *fusedlasso* increases with the number of edges,
which would increase runtime here drastically.
Another possible solution would be to minimize the SST "window" under study (for
example as in @ciemer2020early.)
Our initial motivation was to investigate the global connections between
SST and precipitation, but to reduce computational costs for the fused lasso
it would be practical and investigate the behavior of weighted fully connected
graphs it would be an interesting point of future research.

```{r mse-fused, fig.cap = 'Table of the MSE prediction errors for the different fused lasso models.', fig.align='center'}
knitr::kable(data.frame(c("fused lasso", "sub-graphs removed", "sub-graphs removed, gamma = 0.1", "sub-graphs removed, gamma = 0.05"),
                        c(1131.709,1070.042,1840.589, 1836.632)), 
                        col.names = c("Model", "MSE"),
                        format="latex", caption = "Table of the MSE prediction errors for the different fused lasso models.")
```


