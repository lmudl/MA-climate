# lasso center

```{r, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

```{r}
library(patchwork)
library(ggpubr)
library(raster)
library(glmnet)
library(Hmisc)
source("../code/R/helper-functions.R")
```

```{r}
# inspect error plots
# inspect coef plots
# inspect prediction plots
# length(ids$train$Training060)
# length(ids$test$Testing060)
```
```{r}
# path_to_model_folder <- "../results/CV-lasso/test-deseas-lasso/"
# path_to_model_folder <- "../results/CV-lasso/test-boxcox-lasso/"
# path_to_model_folder <- "../results/CV-lasso/test-lasso-center/"
# path_to_model_folder  <-"../results/CV-lasso/test-log-target-lasso/"
path_to_model_folder <- "../results/CV-lasso/test-deseas-lasso/"

```


## LASSO model

We fit a LASSO model on the precipitation and SST data.
The precipitation target is the monthly mean precipitation
in the Central Amazon Basin, the SST data are monthly temperatures
over the globe.
We use a 5-fold CV approach to find an optimal lambda.
Each fold consists of 5 consecutive years of training data
followed by 2 years of test data.
In each fold we fit a LASSO model on a set of predetermined lambda 
values and choose the lambda that minimizes the MSE on the test set 
in that fold.
After determining the best lambda in each fold we choose the
lambda that minimizes the MSE over all folds and refit the model
to the complete training data. Afterwards we evaluate the fitted model on a separate validation set with 5 years length which was not included
in the training phase.

## Error plots

<!-- ```{r} -->
<!-- err_mat <- readRDS("../results/CV-lasso/cv-lasso-og-data-16-06-22/err-mat.rds") -->
<!-- #err_mat <- sqrt(err_mat) -->
<!-- err_mat <- err_mat[1:60,] -->
<!-- ``` -->

<!-- ```{r} -->
<!-- lambda_id <- c(1:60) -->
<!-- mse <- apply(err_mat, 1, mean) -->
<!-- b <- mse -->
<!-- bplus <- apply(err_mat, 1, max) -->
<!-- bmin <- apply(err_mat, 1, min) -->
<!-- errbar(lambda_id, mse, b+bplus, b-bmin) -->
<!-- ``` -->
```{r}
err_mat <- readRDS(paste0(path_to_model_folder, "/err-mat.rds"))
lambdas <- readRDS(paste0(path_to_model_folder, "/lambda-vec.rds"))
```


```{r err-bar-plot-lasso-og, fig.cap="Mean squared error of the 5-fold blocked cross validation for a range of lambda values on the log scale. The points in the middle represent the average MSE for the respective lambda, the errorbars give the MSE +/- one standard deviation. The dotted line shows the lambda for which minimum MSE was obtained."}
err_bars_plot <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-bars-plot.rds"))
err_bars_plot
a <- ggplot_build(err_bars_plot)
```
\@ref(fig:err-bar-plot-lasso-og) shows the results from the 5 fold-CV plotted for each lambda.
The lambdas are given on the log scale.
The upper and lower bars indicate mean MSE +/- one standard deviation from the
mean MSE. 
We note that the upper and lower bars are quite wide indicating big differences
in MSE for the different folds.
We therefore also inspect the MSE for each individual fold.


<!-- ```{r} -->
<!-- # which.min(b) -->
<!-- # bs <- b[40:100] -->
<!-- # bbpluss <- (b+bplus)[40:100] -->
<!-- # bbmins <- (b-bmin)[40:100] -->
<!-- # (bs > bbpluss) | (bs < bbmins) -->
<!-- bmin <- which.min(b) -->
<!-- tv <- (b+bplus)[bmin] -->
<!-- tz <- b+bplus -->
<!-- tz <- tz[(bmin+1):100] -->
<!-- w <- tv < tz -->
<!-- #bmin is 40 -->
<!-- #w ist bei 45 true -->
<!-- l1se <- 45 -->
<!-- ``` -->



```{r err-fold-lasso-og, fig.cap="MSE of the CV for the different lambda values on the a log scale. The red dotted line shows the lambda for which minimum MSE was obtained."}
p1 <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-plot-fold-1.rds"))
p2 <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-plot-fold-2.rds"))
p3 <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-plot-fold-3.rds"))
p4 <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-plot-fold-4.rds"))
p5 <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-plot-fold-5.rds"))

p1 + p2 + p3 + p4 + p5

```




At first glance \@ref(fig:err-fold-lasso-og) shows that the MSE for fold 1 and 2 have similar trajectories, the same for 3 and 4,
while fold 5 is the only one that only has a local maximum somewhat in the middle of the 
log lambda range. But fold 5 also has its minimum MSE at a larger regularization
value than the other folds which is also reflected in the number of
coefficients it includes in the model, as we will see in the coefficient plots.
Also obviously the MSE differ greatly in their values as can be seen on the differences
of their respective y-axis.
While this plot works well for getting an overview of the trajectories we can
replot them with a common y-axis to compare their values more easily.


```{r}
err_mat_p_list <- list(p1,p2,p3,p4,p5)
err_mat_resc_list <- replot_err_mat(err_mat_p_list)
```

```{r err-fold-lasso-og-replot, fig.cap="MSE of the CV for the different lambda values on the a log scale. The red dotted line shows the lambda for which minimum MSE was obtained. See @\ref(fig:err-folg-lasso-og), but this time the y-axis has the same range for all plots."}
err_mat_resc_list[[1]] + err_mat_resc_list[[2]] + err_mat_resc_list[[3]] + err_mat_resc_list[[4]] + err_mat_resc_list[[5]]
```

We can see now that fold 2 settles for far larger errors than fold 5 for example. Fold 5 chooses the highest lambda but also has the lowest minimal MSE.



Below we can see the minimum MSE for each fold.
```{r}
apply(err_mat, 2, min)
```
And which lambda in the lambda vector resulted in the lowest prediction error on the foldsÂ´ test set.

```{r}
apply(err_mat, 2, function(x) lambdas[which.min(x)])
```

## Coefficient plots

```{r coef-plot-lasso-og, fig.cap="Coefficient map plot for the different folds. Longitude and Latitude on the x and y-axis respectively. Positive values are coloured in blue, negative values in red."}
pc1 <- readRDS(paste0(path_to_model_folder, "/coef-plots/coef-plot-fold-1.rds"))
pc2 <- readRDS(paste0(path_to_model_folder, "/coef-plots/coef-plot-fold-2.rds"))
pc3 <- readRDS(paste0(path_to_model_folder, "/coef-plots/coef-plot-fold-3.rds"))
pc4 <- readRDS(paste0(path_to_model_folder, "/coef-plots/coef-plot-fold-4.rds"))
pc5 <- readRDS(paste0(path_to_model_folder, "/coef-plots/coef-plot-fold-5.rds"))

l <- list(pc1,pc2,pc3,pc4,pc5)
ggarrange(plotlist = l, ncol = 3, nrow = 2,
         common.legend = TRUE)
```

The plots displays the nonzero coefficients in each fold computed
for the lambda that minimizes the MSE on the test set in the respective fold.
The LASSO chooses among correlated variables only one and discards the others,
which can be seen here since the variables chosen are scattered across the map and can but don't have to be close to each other.
<!-- If we recall the standard deviations of the SST (\@ref(fig:mean-and-sd-sst-og)) we can see that when the model is strongly regularized (as in fold 5), it chooses locations that have higher variation in values. Also some of the higher variable locations are not Sea but Great Lake data. -->
<!-- We therefore might consider standardizing the data. -->
<!-- The locations that are included in the models are also not geographically close to the CAB. So we have to ask ourselves if this makes sense from  -->
<!-- a meteorological viewpoint. Can SST far away predict the precipitation in the same month in the CAB? -->
If we take a look again at \@ref(fig:corr-0) we can see that the model includes
locations that have high correlations.


## Inspect predictions from each fold

Following we inspect the folds precipitation time series and
the predictions made by the model.

```{r}
pred_plot_1 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-1.rds"))
#pred_plot_1
mse_1 <- get_mse_from_pred_plot(pred_plot_1)
```
```{r}
pred_plot_2 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-2.rds"))
#pred_plot_2
mse_2 <- get_mse_from_pred_plot(pred_plot_2)

```
```{r}
pred_plot_3 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-3.rds"))
#pred_plot_3
mse_3 <- get_mse_from_pred_plot(pred_plot_3)

```
```{r}
pred_plot_4 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-4.rds"))
#pred_plot_4
mse_4 <- get_mse_from_pred_plot(pred_plot_4)

```
```{r}
pred_plot_5 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-5.rds"))
#pred_plot_5
mse_5 <- get_mse_from_pred_plot(pred_plot_5)

```
```{r}
pred_plot_list <- list(pred_plot_1,pred_plot_2,pred_plot_3,pred_plot_4,pred_plot_5)
#lapply(pred_plot_list, get_mse_from_pred_plot)
```
```{r pred-plot-fold-lasso-og, fig.cap="Precipitation prediction and target values in the test set in each fold. Predictions in red and target values in black."}
pred_plot_1 + pred_plot_2 + pred_plot_3 + pred_plot_4 +
  pred_plot_5
```

```{r}
pred_plot_1 <- readRDS(paste0(path_to_model_folder, "/pred-plots/fit-plot-fold-1.rds"))
#pred_plot_1
mse_1 <- get_mse_from_pred_plot(pred_plot_1)

pred_plot_2 <- readRDS(paste0(path_to_model_folder, "/pred-plots/fit-plot-fold-2.rds"))
#pred_plot_2
mse_2 <- get_mse_from_pred_plot(pred_plot_2)


pred_plot_3 <- readRDS(paste0(path_to_model_folder, "/pred-plots/fit-plot-fold-3.rds"))
#pred_plot_3
mse_3 <- get_mse_from_pred_plot(pred_plot_3)

pred_plot_4 <- readRDS(paste0(path_to_model_folder, "/pred-plots/fit-plot-fold-4.rds"))
#pred_plot_4
mse_4 <- get_mse_from_pred_plot(pred_plot_4)

pred_plot_5 <- readRDS(paste0(path_to_model_folder, "/pred-plots/fit-plot-fold-5.rds"))
#pred_plot_5
mse_5 <- get_mse_from_pred_plot(pred_plot_5)
pred_plot_list <- list(pred_plot_1,pred_plot_2,pred_plot_3,pred_plot_4,pred_plot_5)
#lapply(pred_plot_list, get_mse_from_pred_plot)
```
```{r pred-plot-fold-lasso-og, fig.cap="Precipitation prediction and target values in the test set in each fold. Predictions in red and target values in black."}
pred_plot_1 + pred_plot_2 + pred_plot_3 + pred_plot_4 +
  pred_plot_5
```



In general the model fits the data sufficiently to predict the general
form of the time series but misses some modes and is off in the larger
values in fold 2. Also it does not fit well rapid changes as in fold 3.
Therefore it seems that the model generally underfits the data.

## Inspect predictions from best CV-lambda

A common practice is too choose the largest lambda so that
it`s mean MSE is smaller than the MSE of the lambda that minimizes mean MSE
plus one SE.
But since in our case the largest lambda that satisfies these criteria 
is the maximum lambda we choose the lambda with minimum mean MSE instead.

<!-- ```{r} -->
<!-- # fit model with best lambda -->
<!-- #which.min(b) -->
<!-- lambdas <- readRDS("../results/CV-lasso/cv-lasso-og-data-16-06-22/lambda-vec.rds") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- minmin <- which.min(apply(err_mat, 1, mean)) -->
<!-- #minmin <- l1se -->
<!-- best_lambda <- rev(lambdas)[minmin] -->
<!-- #dim(features) -->
<!-- m_full <- glmnet(features[1:370,], target[1:370], lambda = best_lambda, standardize = FALSE) -->
<!-- preds_full <- predict(m_full, newx = features[371:432,], -->
<!--                       lambda = best_lambda) -->
<!-- df <- data.frame(preds = preds_full, target = target[371:432]) -->
<!-- #dim(preds_full) -->
<!-- ggplot() + geom_line(data = df, mapping = aes(x=1:62, y=s0, colour = "blue")) + -->
<!--   geom_line(data = df, mapping= aes(x=1:62, y=target)) -->

<!-- ``` -->
<!-- ```{r} -->
<!-- sqrt(mean((df$s0 - df$target)^2)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- mean((df$s0 - df$target)^2) -->
<!-- ``` -->




```{r pred-plot-full-lasso-og, fi.cap="Precipitation prediction and target values in the validation set. Predictions in red and target values in black. The model was fitted on the full CV data with the lambda value that minimised the average MSE"}
full_preds <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-full.rds"))
full_preds
mse_full <- get_mse_from_pred_plot(full_preds)
mse_full
```
```{r}
coef_full <- readRDS(paste0(path_to_model_folder,
                     "coef-plots/coef-plot-full.rds"))
coef_full
```



```{r}
full_best <- readRDS(paste0(path_to_model_folder,
                         "pred-plots/best-pred-plot-full.rds"))
full_best
```


```{r}
full_fit <- readRDS(paste0(path_to_model_folder, "pred-plots/fitted-preds-plot-full.rds"))
full_fit
```


```{r}
full_all_err <- readRDS(paste0(path_to_model_folder, "full-model-all-errors-plot.rds"))
full_all_err
```

Over the more than 5 years of validation data the model predicts the
seasonal pattern of the precipitation time series quite well,
but constantly fails to predict the higher values of precipitation.
The MSE is `mse_full` and the RSME `sqrt(mse_full)`.

## Summary
We fitted a LASSO model for predicting the mean precipitation
in the Central Amazon Basin and used a 5-fold blocked Cross Validation
approach to find the optimal level of regularization.
After training the model we evaluated its performance on a separate validation set that was not used in the training process.
The model shows predicting capabilities but misses out on higher values of the precipitation target. It also misses on rapid changes and in general underfits the data.
This may be due to the choice of blocked cross validation.
<!-- Also although predicting seems to bring useful results, the choice of the LASSO coefficients seems somewhat arbitrary or at least not -->
<!-- interpret able in a straightforward way. -->
Locations with higher variability get included in the model more easily
and are not necessarily geographically close.


<!-- ```{r} -->
<!-- m_full2 <- glmnet(features[1:370,], target[1:370], lambda = rev(lambdas), standardize = FALSE) -->
<!-- preds_full2 <- predict.glmnet(m_full2, newx = features[371:432,], s = rev(lambdas)) -->
<!-- v <- apply(preds_full2,2,function(x) mean((x-target[371:432])^2)) -->
<!-- v <- unname(v) -->
<!-- best_l <- which.min(v) -->
<!-- df2 <- data.frame(preds = preds_full2[,best_l], target = target[371:432]) -->
<!-- #dim(preds_full) -->
<!-- ggplot() + geom_line(data = df2, mapping = aes(x=1:62, y=preds, colour = "blue")) + -->
<!--   geom_line(data = df, mapping= aes(x=1:62, y=target)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- sqrt(mean((df2$preds - df$target)^2)) -->
<!-- ``` -->

```{r}
# save full model after fitting!
df <- ggplot_build(full_preds)
d <- df$data
a <- d[[1]]$y # fitted value
b <- d[[2]]$y # 
res <- a-b # residuals

plot(a,res)
qqnorm(res)
qqline(res)
plot(hist(res))
```

```{r}
acf(res)
Box.test(res, lag=20, type="Ljung-Box")

```
```{r}
plot.ts(res)
```
```{r}
pacf(res)
```


```{r}
plot(hist(res))
```
```{r}
# fm <- readRDS(paste0(path_to_model_folder,"/full-model.rds"))
# err_bars_plot
# op_l <- a$data[[3]]$xintercept
# xind <- which(a$data[[2]]==op_l)
# maxim <- a$data[[2]]$ymax[xind]
# maxim
# mm_vec <- a$data[[1]]$y
# mm_vec < maxim 
# # lambda 71 von der negativen Seite
# lnew <- lambdas[71]
# # xintercept optimal lambda 1.28
# p_eval <- readRDS("../data/processed/precip_eval.rds")
# s_eval <- readRDS("../data/processed/sst_eval.rds")
# # get ymax at optimal lambda
# # get smallest lambda so that mse is smaller than ymax
# preds_new <- c(predict(fm, newx = s_eval, s=lnew))
# comp_mse(preds_new, p_eval)
# plot(ts(p_eval))
# plot(ts(preds_new))
# df <- data.frame(predictions=preds_new,targets=p_eval, ids=1:62)
# plot_predictions(df)

```
```{r}
# err_bars_plot

```


