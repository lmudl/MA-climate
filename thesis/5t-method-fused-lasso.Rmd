# Fused Lasso Regression

## The fused lasso

```{r}
# TODO add statistical learning with sparsity to references.
```
```{r}
# TODO add to summary of LASSO defaults, that characteristic 
# of LASSO
```


As expected and seen in the results, the different LASSO models choose single
SST regions as predictors as opposed to whole regions.
Since the LASSO only regularizes the magnitude of coefficients
but ignores the their ordering. 
We therefore use the so-called *fused lasso* as implemented in the *genlasso* package and the respective *fusedlasso* function (\@ref{genlassopackage}, \@ref{thibshirani2005sparsity}).

```{r}
# TODO make reference to achieved results here
```

```{r}
# TODO cite general LASSO properties here
# how it acts in situations where coefficients are grouped
# or highly correlated.
```

```{r}
# TODO add plot of the graph given by our helper functions.
```

```{r}
# TODO add graphic of fig.2 fused lasso
```

The fused LASSO solves the problem:

$$\min_{\beta} 1/2 \sum_{i=1}^n(y_i - x_i^T\beta_i)^2 + \lambda \sum_{i,j \in E} |\beta_i - \beta_j| + \gamma \cdot \lambda \sum_{i=1}^p|\beta_i|  ,$$
with $x_i$ being the ith row of the predictor matrix and 
E is the edge set of an underlying graph.
Regularizing $|\beta_i-\beta_j|$, penalizes large differences in
close coefficients. In our case "close" means small distances
as defined on 2-dimensional longitude/latitude grid. This grid defines a graph that can be used to compute the differences for
each location.
The third term $\gamma \cdot \lambda \sum_{i=1}^p|\beta_i|$,
controls the sparsity of the coefficients. $\gamma=0$ leads to complete fusion of the coefficients (no sparsity) and $\gamma$ > 0 introduces sparsity to the solution, with higher values placing more priority on sparsity. 
$\hat{\beta}$ is computed as a function of $\lambda$, with fixed $\gamma$.


```{r}
#TODO note the fused lasso paper here are
```

```{r}
# TODO add efficient implementations of hte generlaized lasso dual path
# algorithm to references
```

```{r}
# TODO ?check generalized dual path algorithm og paper?
```

```{r}
# Efficient Implementations of the Generalized Lasso Dual Path
# Algorithm
```

In the fused lasso setting the coefficients $\beta \in \mathbb{R}^p$ can be thought of
as nodes of a given undirected Graph $G$, with edge set $E \subset {1,...,p}^2$.
Now lets assume that $E$ has $m$ edges which are enumerated $e_1,...,e_m$.
The fused lasso penalty matrix $D$ is then $m \times p$, where 
each row corresponds to an edge in $E$.
So when $e_l = (i,j)$, we write $l_th$ row of $D$ as 

$$ D_l = (0,...-1,...1,...) \in \mathbb{R}^p, $$
meaning $D_l$ has all zeros except for the the $i_th$ and $j_th$
location.




# quick summary what I got so far from the algorithm
1.2 case X=I.
very general dual path algorithm
then the use general X, here they compute moore-penrose pseudoinverse
and substitute Xtilde and ytilde
Also when p>n X does not have full column rank they add a diagonal matrix
to the rows, with eps time beta magnitude on the diagonals (see github)

implementation: do not solve least square problems all the time,
but use QR decomposition in the beginning and then later 
update the QR decomposition.
in some cases more meaningful to take advantage of special structures of D. In our case they use Laplacian linear systems

QR-based general X
computing D tilde = DX+ destroy special structures in D,
need other methods see Section 5
NOTE: total number of steps are not understood really
they use direct solvers because past decisions influence future outcome

2. QR-based general D
QR in appendix but role of m and n are changed.
two strategies: wide and tall
our case tall strategy
DPG=QR, rotated QR decomposition

4. Special implementation for fused lasso, X=I
computing DDT are highly sparse but. are underdetermined
because m larger than n, more edges than nodes.
We can find arbitrary solution (we will called it basic solution)
and from arbitrary solution we can find solution with minimum l2 norm.
4.1 There is not necesserily a computional improvement but for
special cases as for example fused lasso there are improvements
because of special structure in D.
they compute projection onto the null and basic solutions of linear systems, then from basic solution, optimal solution.


# what to they actually use now


They change algorithm 2 becaus using xtild and stuff because are options are needed when D has structure using xtilde will destroy special structure in D.
So when X general we use xtilde but using xtilde will destroy structure in D, therefore when X general AND D has structure use following:


### argumentation
They use gen lasso framework for writing it down
related work, dual in general problem, primal did not solve that
dual is not distinct
all lambdas therefore path algorithm
write down algorithm to solve
for alg 2 general x use d tilde and x tilde
if x is rank defficient add ridge penalty (see formula in og paper)
if x is identity they use qr instead of linear system solvers
they are direct and can be cleverly updated
use direct solvers because decisions fom past influence future decisions
if D has special structure they propose special implementations
for fused lasso problem this means that sparse matrices have to 
be solved. arbitrary solutions finding easy min l2 norm is harder,in general.
they show how to compute l2 norm solutions from arbitrary solution.
Using the special structure for (sparse) fused lasso problem
of D, computing projections onto null and solving laplacian
systems. because if D penalty matrix of undirected graph
DTD is laplacian.


show alg in general
sho alg for general x
  explain what to do when rank defficient
but then use specialized implementation
  explain DTD is sparse but solution has to be found 
    with little trick
  DTD is laplacian system and can be solved by cholesky.








