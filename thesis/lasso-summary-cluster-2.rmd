#### Cluster 2 {#cl2}

```{r, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE, message = FALSE, out.width = '50%', fig.align = 'center')
options(knitr.duplicate.label = "allow")
```

```{r}
library(patchwork)
library(ggpubr)
library(raster)
library(glmnet)
library(Hmisc)
library(patchwork)
source("../code/R/helper-functions.R")
```

```{r}
path_to_model_folder <- "../results/CV-lasso/cluster-cv-lasso-og2/cluster-2/"
err_bars_plot <- readRDS(paste0(path_to_model_folder, "/err-mat-plots/err-bars-plot.rds"))
p1 <- readRDS(paste0(path_to_model_folder, "err-mat-plots/err-plot-fold-1.rds"))
p2 <- readRDS(paste0(path_to_model_folder, "err-mat-plots/err-plot-fold-2.rds"))
p3 <- readRDS(paste0(path_to_model_folder, "err-mat-plots/err-plot-fold-3.rds"))
p4 <- readRDS(paste0(path_to_model_folder, "err-mat-plots/err-plot-fold-4.rds"))
p5 <- readRDS(paste0(path_to_model_folder, "err-mat-plots/err-plot-fold-5.rds"))
pred_plot_1 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-1.rds"))
pred_plot_2 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-2.rds"))
pred_plot_3 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-3.rds"))
pred_plot_4 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-4.rds"))
pred_plot_5 <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-fold-5.rds"))
full_preds <- readRDS(paste0(path_to_model_folder, "/pred-plots/pred-plot-full.rds"))
coef_plot_full <- readRDS(paste0(path_to_model_folder,
                                 "coef-plots/coef-plot-full.rds"))
full_model <- readRDS(paste0(path_to_model_folder, "full-model.rds"))
intercept2 <- round(full_model$a0,2)
lambda2 <- full_model$lambda
mse2 <- get_mse_from_pred_plot(full_preds)
```

```{r cl2-err-bar, fig.cap="Model: Lasso on cluster 2. Mean squared error of the 5-fold blocked cross validation for a range of lambda values on the log scale. The points in the middle represent the average MSE for the respective lambda, the errorbars give the MSE +/- one standard deviation. The dotted line shows the lambda for which minimum MSE was obtained."}
err_bars_plot + ggtitle("MSE for lasso on cluster 2")
```

\@ref(fig:cl2-err-bar) has more narrow error bars
than the same plot for cluster 1. It has optimum at
`r log(lambda2)` and a minimum mean MSE is `r mse2`.
And regularization chosen here is lower and the resulting MSE as well.


```{r cl2-full-pred, fig.cap="Model: Lasso on cluster 2. Precipitation prediction and target values in the validation set. Predictions in red and target values in black. The model was fitted on the full CV data with the lambda value that minimised the average MSE"}
full_preds + ggtitle("Predictions on evaluation set, lasso on cluster 2")
```

The final predictions on cluster 2 catch the seasonality as well as high and low values 
reasonably well (\@ref(fig:cl2-full-pred)). 
But it still misses higher values in the 4th and 5th peak

```{r cl2-coef-plot, fig.cap=paste("Model: Lasso on cluster 2. Coefficient plot of the full model. Fitted intercept of", intercept2)}
coef_plot_full + ggtitle("Coefficient plot, lasso on cluster 2") + theme(legend.position = "bottom")
```
The lower $\lambda$ chosen in cluster 2 results
in a higher number of non-zero coefficients
than for example in cluster 1 (\@ref(fig:cl2-coef-plot)).
Here the intercept is also higher (`r intercept2`) than
the predicted values. So most of the coefficient values are negative.

