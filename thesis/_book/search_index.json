[["index.html", "Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures Introduction", " Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures Dario Lepke Introduction With future climate change droughts in the Amazon forest may become more frequent and/or severe. Droughts can turn Amazon regions from rain forest into savanna, leading to high amounts of carbon released into the atmosphere. Therefore, predicting future droughts and understanding the underlying mechanisms is of great interest. @ciemer2020early, established an early warning indicator for droughts in the central Amazon basin (CAB), based on tropical Atlantic sea surface temperatures (SSTs). In my thesis I would like to build on this work and improve the predictive power by using different statistical methods. Meaning, we seek to build a model that is able to predict droughts (resp. rainfall) based on preceding sea temperatures, desirably with as much lead time as possible. Also we want to identify those sea regions that are most important for doing so, making interpretability a point of interest, too. A first model could be a cross-validated (generalised) LASSO approach trying to identify the most important oceanic regions and the respective time-scales. The thesis will be done in cooperation with Dr. Niklas Boers from the Postdam Institute for Climate Impact Research (@PIC). "],["related-work-ciemer2020early.html", "Related work @ciemer2020early Summary", " Related work @ciemer2020early As already mentioned the paper by @ciemer2020early, created an early warning indicator for Amazon droughts. They did so using a complex network approach. They used two datasets, one for the Sea Surface Temperatures (@smith2008improvements) and one for the precipitation (@funk2015climate) with monthly data for the time period of 1981 until 2016. The data can be downloaded for example in netcdf format and manipulated conventiently with Climate Data Operators (CDO, @schulzweida2019cdo). CDO in turn can be used with wrappers for R and Python. The data is organized on a longitude/latitude grid. They identify 4 oceanic regions that correlate the most with rain in the amazon basin, using a coupled network approach. Figure 0.1 shows the cross degree towards rainfall in the central Amazon basin (CAB, blue box), for positive and negative correlations. Darker shades indicate a larger cross degree, hence a larger number of links and correlations with rainfall at more grid points in the CBA. The correlations are measured using a spearman rank-order correlation coefficient \\[ \\rho = 1 - \\frac{6\\sum\\Delta_{R_i}^2}{n(n^2-1)} . \\] Where \\(\\Delta_{R_i}\\) denotes the difference between the ranks of observations of both variables at the same time \\(i\\) and \\(n\\) is the number of observations. An Adjacency Matrix describes the resulting network, where the threshold \\(p_{th}\\) was chosen so that only 10% of the strongest correlations are represented as links in the network \\[ A_{ij} = \\begin{cases}0 \\textrm{ } p_{ij} &lt; p_{th}\\\\ 1 \\textrm{ } p_{ij}\\geq p_{th}\\\\\\end{cases} . \\] The cross degree then gives the strength of correlation between a specific grid point \\(i\\) of network \\(V_l\\) (oceanic grid point) and another (sub)network \\(V_m\\) (all grid points \\(j\\) in central amazon basin) \\[ k_{i}^{lm} = \\sum\\limits_{j \\in V_m} A_{ij}, i \\in V_l\\ . \\] Figure 0.1: Cross degree between sea surface temperature and continental rainfall anomalies. For each sea surface temperature grid cell of the Atlantic and Pacific Ocean, the cross degree towards rainfall in the Central Amazon Basin (blue box) is shown, for a positive correlations and b negative correlations. Darker shading indicates a larger cross degree, implying a larger number of links, and thus significant correlations with rainfall at more grid points in the Central Amazon Basin. Red areas outline coherent oceanic regions with a the 20% highest cross degrees for positive correlations, found in the Southern Pacific Ocean (SPO) and Southern Tropical Atlantic Ocean (STAO), and b the 20% highest cross degrees for negative correlations, found in the Central Pacific Ocean (CPO) and Northern Tropical Atlantic Ocean (NTAO) [@ciemer2020early] They further explore the relationship by constructing (weighted) networks for sliding windows of 24 months between the Central Amazon Basin and each of the ocean regions. For each month except for the first two years, an individual network is computed based on the data of the previous 24 months. Then they take the average of the cross correlations for each of the networks which gives a new time series of average cross correlation (ACC) values. Each ACC summarizes the connectivity of one region with the CAB for the last 24 months. They find that NTAO and STAO give the strongest signal, hence they apply the same sliding window coupled network approach between the ocean regions NTAO and STAO. Before they computed networks between ocean and continental regions, now it is computed between these two atlantic regions, NTAO and STAO. The resulting time series and its comparison to the drought index time series is shown in figure 0.2 below. They find that using a ACC threshold for a drought (SPI below -1.5), lets them forecast 6 of the 7 droughts in the observation period, missing the 2005 drought, while also giving one false alarm in 2002. Figure 0.2: Early-warning signal for droughts in the central Amazon basin. We compare the time evolution of the average cross correlation of the Northern Tropical Atlantic Ocean (NTAO) and Southern Tropical Atlantic Ocean (STAO), given by the blue curve, with the standardized precipitation index (SPI, orange) of the central Amazon basin. Orange dips indicate a negative SPI with a threshold for severely dry periods (SPI -1, dotted red line). We expect a drought event within the following one and a half years whenever the average cross correlation between NTAO and STAO SST anomalies falls below an empirically found threshold of -0.06. Green circles indicate a matching forecast based on the Atlantic SST correlation structure, with one false alarm in 2002 indicated by a grey circle, where the threshold is crossed but no drought took place in the direct aftermath (see Discussion). The temporal evolution of the average cross correlation shown here is smoothed using a Chebyshev type-I low-pass filter with a cutoff at 24 months [@ciemer2020early]. Summary The work by @ciemer2020early shows potential forecasting capabilities and limitations. While they are able to predict 5 out of 6 drought events, they also give one false negative and one false positive result. Their work uses a complex network approach that is applied stepwise (first two unweighted networks, then two weighted networks and in the end a dichotomous threshold decision rule). For the thesis we would like to create a more general predictive model that can learn the relationship between the SSTs and rainfall in the CAB. As already mentioned a first step can be a LASSO model. First findings show that, the classic LASSO only chooses single points in the ocean as predictors, though. But our motivation is to discover predictive regions and not only single seperated points. Therefore in a next step we want to make use of a generalised form of the LASSO that also takes into account that chosen predictors should be close to each other. This model is the so called Fused LASSO. For the models we also need a form of evaluation. Classic Cross Validation assumes independence of the observations. In our setting this is clearly violated due to the time dependency of the data. We will explore different possibilities to use an adjusted form of Cross Validation that takes this characteristic into account. Depending on how well the relationship between SST and rain can be established, we can take this a step further and use it as a so called Emergent Constraint (EC). Since different climate models give different answers about future climate there is a need to narrow this spread, which can be done by ECs. To do so, we need a plausible relationship between a Variable X and Y (here: SST and drought). According to how well the relationship is represented in a climate model we assign credibility to a climate models future projections (here: projections of future droughts in the Amazon rain forest). In summary this can be used to reduce uncertainty in the ensemble of climate models future projections, f.e by using ML techniques as done by @schlund2020constraining. In this chapter we will first summarize the main ideas of clustering and then apply it to the precipitation data. If not indicated otherwise the information is taken from Elements of Statistical Learning. "],["main-idea-clustering.html", "1 Main Idea Clustering", " 1 Main Idea Clustering We can describe an object by a set of measurements or its similarity to other objects. Using this similarity we can put a collection of objects into subgroups or clusters. The objects in the subgroups should then be more similar to one another than to objects of different subgroups. This means inside the clusters we aim for homogeneity and for observations of different clusters for heterogeneity. With the clustering analysis applied to the precipitation data we want to study if there are distinct groups (regions) apparent in the CAB. So that if we later apply the regression models we predict the precipitation for each group and not for the whole region. To explore the grouping in the data we need a measure of (dis)similarity. This measure is central and depends on subject matter considerations. We construct the dissimilarities based on the measurements taken for each month. We interpret this as a multivariate analysis where, each month is one variable. So given the area in the CAB (resolution 5°x5°), we have 612 cells and 432 months, resulting in a \\(612 \\times 432\\) data matrix. we want to cluster cells into homogen groups. "],["clustering-methods.html", "2 Clustering Methods 2.1 K-means 2.2 K-medoids 2.3 PCA (Multivariate Verfahren und ESL2) 2.4 Gap statistic", " 2 Clustering Methods 2.1 K-means In the following we briefly describe the K-means procedure. Beforehand we have to specify a number of clusters \\(C\\) we believe exist in our data. Then we randomly initialize \\(C\\) cluster centers in the feature space. Now two steps are repeated until convergence: For each center we identify the points that are the closest to this center. These points belong now to a cluster \\(C\\). In each cluster we compute the mean of each variable and get a vector of means. This mean vector is now the new center of the cluster. As a measure of dissimilarity we use the Euclidean distance: \\[ d(x_i,x_{i´}) = \\sum_{i=1}^p(x_{ij}-x_{i´j})^2=||x_i-x_{i´}||^2 \\] Meaning for the points \\(i\\) and \\(i´\\) we compute the squared difference for each variable and sum them up. As stated above we are searching for clusters that are themselves compact, meaning homogeneous. We do so by minimizing the mean scatter inside the clusters. We summarize this scatter as within-sum-of-squares \\[ W(C) = \\frac{1}{2} \\sum_{k=1}^{K} \\sum_{C(i)=k} \\sum_{C(i´)=k} ||x_i-x_{i´}||^2 \\\\ = \\sum_{k=1}^K N_k \\sum_{C(i)=k} ||x_i-\\bar{x}_k ||^2 \\] where \\(\\bar{x} = (\\bar{x}_{1k},...,\\bar{x}_{pk})\\) stands for the mean vectors of the \\(k\\)th cluster and \\(N_k = \\sum_{i=1}^N I(C(i)=k)\\). 2.1.1 Kmeans characteristics variance of each distribution of each attribute (variable) is spherical, variance is symmetrical? all variables have same variance, not the case in our example, therefore scaling or pca equal number of observations in each clusters, we don`t know Since we use the Euclidean distance the similarity measures will be sensitive to outliers and scale. K-means assumes that the variance of a variable´s distribution is spherical, meaning it wight not work well in situations that violate this assumptions (f.e non-spherical data). Further assumptions are same variance of the variables, and equally sized clusters. Now how large these violations have to be so that k-means does not work well anymore has no clear-cut answer. 2.2 K-medoids We can adjust k-means procedure so that we can use other distances than the Euclidean distance. The only part of the k-means algorithm that uses Euclidean distance is when we compute the cluster centers. We can replace this step by formalizing an optimization with respect to the cluster members. For example so that each center has to be one of the observations assigned to the cluster. K-medoids is far more computationally intensive than K-means. 2.2.1 K-medoids characteristics K-medoids is Less sensitive to outliers, because it minimizes sum of pairwise dissimilarities instead of sum of squared Euclidean distances. As stated above it is also more computationally intensive. 2.3 PCA (Multivariate Verfahren und ESL2) Goal is reduction of correlated and eventually large number p variables to a few. We accomplish this by creating new variables that are linear combinations of the original ones. We call these new variables principle components. The new variables are not correlated any more and ordered according to the variance they explain. The first \\(k &lt; p\\) principal components then contain the majority of variance. (Multivariate Verfahren) As they are ordered they also provide a sequence of best linear approximations of our data. Let \\(x_1, x_2,...,x_N\\), be our observations and we represent them by a rank-q linear model \\[ f(\\lambda) = \\mu + V_q\\lambda\\] with \\(\\mu\\) a location vector in \\(\\mathbb{R}^p\\) and \\(V_q\\) is a \\(p \\times q\\) matrix with columns being orthogonal unit vectors as columns. \\(\\lambda\\) is a \\(q\\) vector of parameters. In other words we are trying to fit a hyperplane of rank q to the data. If we fit this model to minimize reconstruction error using least squares we solve \\[ \\min_{\\mu, \\{\\lambda_i\\}, V_q } \\sum_{i=1}^N||x_i - \\mu - V_q\\lambda_i ||^2 \\] If we partially optimize for \\(\\mu\\) and \\(\\lambda_i\\) we obtain \\[ \\hat{\\mu} = \\bar{x},\\] \\[\\hat{\\lambda_i} = V_q^T(x_i - \\bar{x}) \\] Therefore we need to search for the orthogonal matrix \\(V_q\\) \\[ \\min_{V_q} \\sum_{i=1}^N ||(x_i-\\bar{x}) - V_qV_q^t(x_i-\\bar{x})||^2 \\] We can assume here that \\(\\bar{x}\\) is 0, if this not the case we can simply center the observations \\(\\tilde{x}_i = x_i - \\bar{x}\\). \\(H_q = V_qV_q^T\\) projects each observation \\(x_i\\) from the original feature space onto the subspace that is spanned by the columns of \\(V_q\\). \\(H_qx_i\\) is then the orthogonal projection of \\(x_i\\) Hence \\(H_q\\) is also called the projection matrix. Stackoverflow post (https://math.stackexchange.com/questions/3982195/what-are-left-and-right-singular-vectors-in-svd) We find \\(V_q\\) then by constructing the singular value decomposition of our data matrix X. \\(X\\) contains the (centered) observations in rows, giving a \\(N \\times p\\) matrix. The SVD is then: \\[ X = UDV^T \\] Where \\(U\\) is an orthogonal matrix containing the left singular vectors \\(u_j\\) as columns, and \\(V\\) contains the right singular vectors \\(v_j\\). The columns of \\(U\\) span the columns space of \\(X\\) and the columns of \\(V\\) span the row space. \\(D\\) is diagonal matrix which contains singular values, \\(d_1 \\leq d_2 \\leq ... \\leq d_p \\leq 0\\). Singular values are square roots of non-negative eigenvalues. The columns of \\(UD\\) are the principal components of \\(X\\). So the SVD gives us the matrix \\(V\\) (the first \\(q\\) columns give the solution to the minimization problem above) as well as the principal components from \\(UD\\). 2.4 Gap statistic (see gap statistic in plain english stackoverflow) (see clusgap paper) As stated above we usually measure how compact our clusters are by assessing \\(W(C)\\) or \\(log(W_c)\\). Where are a low value indicates compact clusters. To compare the value we need a reference. We therefore want to estimate how large \\(W_c\\) were if there were no clusters present in our data. The larger the difference between the \\(W_c\\) from the data and the one from the reference the more likely we are to say that the found number of clusters is indeed correct. We construct reference data by sampling from a uniform distribution based on our data. Say we have \\(p\\) variables. We sample \\(n\\) times from each of the \\(p\\) uniformly distributed variables, where maximum and minimum are obtained from our data. We then cluster the reference data in the same we cluster our observed data and compute \\(W_c\\). We repeat this process several times and compute the average of \\(W_c\\), \\(E \\{log(W_c)\\}\\). The gap statistic is then the difference \\[E \\{log(W_c) \\} - log(W_c)\\]. So in cases where our data is formed of clusters we would expect a high gap statistic. As Tibshirani et al. note and as it is also done in the used R function clusgap, doing a PCA on the data and compute the gap statistic on the PCA scores can improves the results of gap statistic. "],["analyse-clustering-results.html", "3 Analyse clustering results", " 3 Analyse clustering results We now analyze further the results we found. Namely the clusters we find after performing a PCA on the data, using the 3 first principal components and searching for 5 clusters using k-means. The first 3 principal components explain 67.8% of the variance We found 5 as optimal number of clusters based on the gap statistic and the criteria from Tibsherani. We show the map plot for the k-means clustering after applying the PCA as well as the time series of the resulting clusters. km_pca_map_plot + ggtitle(&quot;Precipitation clusters in the CAB, after centered PCA, using 3 PC&#39;s and applying kmeans&quot;) The plot above shows the grid cells in the Central Amazon Basin colored according to the clusters that are assigned by k-means. The clusters are almost completely spatially coherent. Meaning that the clusters are not scattered across different areas. One exception can be seen for Cluster 1 and 4. Parts of cluster 1 (orange) are inside cluster 4 (blue) and on the edge to cluster 3 (green). c1_pca_km + c2_pca_km + c3_pca_km + c4_pca_km + c5_pca_km We now inspect the original (centered) time series inside the clusters. The time series are shown in gray and the monthly mean in the cluster is shown in blue. Since the time series are centered before applying the PCA and clustering, the zero value is the mean of the respective month of the whole CAB. The clusters differ in their monthly differences from the monthly CAB mean (here 0 because the time series were centered before PCA and k-means) and their fluctuation/ variance. Also the size of the clusters are not all the same. The mean in cluster 3 has lowest variability around the CAB mean, followed by clusters 2 and 5, and clusters 1 and 4 have the highest variability. On average cluster 3 is on the level of the CAB overall mean. The clusters 2 and 5 are slightly below and cluster 4 is above the CAB mean, on average. Cluster 1 is on average also on the CAB mean but shows more variance than cluster 3. "],["clustering-results.html", "4 Clustering results 4.1 With centering 4.2 Without centering 4.3 Summary", " 4 Clustering results We summarize and compare the results we obtain if we center or dont center the data. 4.1 With centering 4.1.1 Kmeans and PAM gap statistics without PCA gap_kmeans_centered_plot + gap_pam_centered_plot We can see that on the original data, neither K-means nor PAM find an optimal number of clusters that is lower than the maximum number specified. 4.1.2 Scree plot scree_plot_pca_centered &lt;- readRDS(&quot;../results/scree_plot_pca_centered.rds&quot;) scree_plot_pca_centered We apply a PCA on the centered data. The variance that is explained by the principal components indicates that, the 3 principal components already explain a lot of the appearing variance in the data. We now study the gap statistic results for K-means and K-medoids (here, meaning PAM) after applying the PCA and choosing the number of principal components to be used. Following we compare the results of a gap statistic when the first 3 and 4 principal components are used, for K-means and PAM respectively. 4.1.3 K-means and PAM gap statistics after applying PCA summary_plot_centered &lt;- readRDS(&quot;../results/clustering/summary_plot_pca_centered.rds&quot;) summary_plot_centered The graphic shows the gap statistics resulting from K-means and PAM for 3 and 4 principal components respectively. For 3 principal components K-means and PAM settle for 12 as optimal number of clusters. For 4 principal components K-means chooses 16 and PAM the maximum number possible here 20. 4.2 Without centering 4.2.1 K-means and PAM gap statistics without PCA gap_kmeans_plot + gap_pam_plot As for the centered data, K-means and PAM choose the maximum number of possible clusters as optimal. 4.2.2 Scree plot scree_plot_pca_uncentered &lt;- readRDS(&quot;../results/scree_plot_pca_uncentered.rds&quot;) scree_plot_pca_uncentered We now apply a PCA on the uncentered data. The first principal component now explains a lot more variance. See https://eigenvector.com/wp-content/uploads/2020/06/EffectofCenteringonPCA.pdf We proceed as for the centered data but use 2 and 3 principal components respectively 4.2.3 Kmean and PAM gap statistics after applying PCA summary_plot_pca_uncentered &lt;- readRDS(&quot;../results/clustering/summary_plot_pca_uncentered.rds&quot;) summary_plot_pca_uncentered The graphic shows the gap statistics resulting from K-means and PAM for 2 and 3 principal components respectively. For 2 principal components K-means and PAM both choose 3 as the optimal number of clusters, while for 3 principal components hey choose 12. The gap statistic spike for 2 principal components is more distinct. 4.3 Summary We used the gap statistic as evaluation tool to find the optimal number of clusters in our data. It was applied by using K-means and PAM as cluster algorithms, on the raw and centered data with and without applying a pca beforehand. When applied without PCA, PAM and K-means found the maximal number of clusters optimal, regardless of centering. The results differed regarding centering the data when a PCA was applied. When the data was centered before the PCA, we find 12 (3 principal components, both K-means and PAM) and 16 or 20 (4 principal components, K-means and PAM respectively) as optimal number of clusters. Without centering, we find 3 and 12 as optimal number of clusters (2 and 3 principal components, K-means and PAM agree). The results overall vary greatly and are sensitive to choices such as centering. The value of the clustering itself can be evaluated only when used in combination with regression. If useful, fitting different models for the different clusters should result in a lower overall average prediction error. We will proceed by using K-means or PAM for finding 3 clusters after applying a PCA on the uncentered data and use 2 principal components, since the results for this procedure gave more clear cut results. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
