<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Clustering Methods | Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures</title>
  <meta name="description" content="6 Clustering Methods | Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Clustering Methods | Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Clustering Methods | Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures" />
  
  
  

<meta name="author" content="Dario Lepke" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="main-idea-clustering.html"/>
<link rel="next" href="clustering-results.html"/>
<script src="libs/header-attrs-2.12/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="index.html#introduction">Introduction<span></span></a></li>
<li class="chapter" data-level="1" data-path="related-work.html"><a href="related-work.html"><i class="fa fa-check"></i><b>1</b> Related work<span></span></a></li>
<li class="chapter" data-level="2" data-path="eda-precipitation.html"><a href="eda-precipitation.html"><i class="fa fa-check"></i><b>2</b> EDA precipitation<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda-precipitation.html"><a href="eda-precipitation.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="eda-precipitation.html"><a href="eda-precipitation.html#mean-at-each-location"><i class="fa fa-check"></i><b>2.2</b> Mean at each location<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="eda-precipitation.html"><a href="eda-precipitation.html#sd-at-each-location"><i class="fa fa-check"></i><b>2.3</b> SD at each location<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="eda-precipitation.html"><a href="eda-precipitation.html#trend-at-each-location"><i class="fa fa-check"></i><b>2.4</b> Trend at each location<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="eda-precipitation.html"><a href="eda-precipitation.html#means-per-month"><i class="fa fa-check"></i><b>2.5</b> Means per month<span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="eda-precipitation.html"><a href="eda-precipitation.html#january"><i class="fa fa-check"></i><b>2.5.1</b> January<span></span></a></li>
<li class="chapter" data-level="2.5.2" data-path="eda-precipitation.html"><a href="eda-precipitation.html#february"><i class="fa fa-check"></i><b>2.5.2</b> February<span></span></a></li>
<li class="chapter" data-level="2.5.3" data-path="eda-precipitation.html"><a href="eda-precipitation.html#march"><i class="fa fa-check"></i><b>2.5.3</b> March<span></span></a></li>
<li class="chapter" data-level="2.5.4" data-path="eda-precipitation.html"><a href="eda-precipitation.html#april"><i class="fa fa-check"></i><b>2.5.4</b> April<span></span></a></li>
<li class="chapter" data-level="2.5.5" data-path="eda-precipitation.html"><a href="eda-precipitation.html#may"><i class="fa fa-check"></i><b>2.5.5</b> May<span></span></a></li>
<li class="chapter" data-level="2.5.6" data-path="eda-precipitation.html"><a href="eda-precipitation.html#june"><i class="fa fa-check"></i><b>2.5.6</b> June<span></span></a></li>
<li class="chapter" data-level="2.5.7" data-path="eda-precipitation.html"><a href="eda-precipitation.html#july"><i class="fa fa-check"></i><b>2.5.7</b> July<span></span></a></li>
<li class="chapter" data-level="2.5.8" data-path="eda-precipitation.html"><a href="eda-precipitation.html#august"><i class="fa fa-check"></i><b>2.5.8</b> August<span></span></a></li>
<li class="chapter" data-level="2.5.9" data-path="eda-precipitation.html"><a href="eda-precipitation.html#september"><i class="fa fa-check"></i><b>2.5.9</b> September<span></span></a></li>
<li class="chapter" data-level="2.5.10" data-path="eda-precipitation.html"><a href="eda-precipitation.html#october"><i class="fa fa-check"></i><b>2.5.10</b> October<span></span></a></li>
<li class="chapter" data-level="2.5.11" data-path="eda-precipitation.html"><a href="eda-precipitation.html#november"><i class="fa fa-check"></i><b>2.5.11</b> November<span></span></a></li>
<li class="chapter" data-level="2.5.12" data-path="eda-precipitation.html"><a href="eda-precipitation.html#december"><i class="fa fa-check"></i><b>2.5.12</b> December<span></span></a></li>
<li><a href="eda-precipitation.html#section"></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="eda-precipitation.html"><a href="eda-precipitation.html#sd-per-month"><i class="fa fa-check"></i><b>2.6</b> SD per month<span></span></a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="eda-precipitation.html"><a href="eda-precipitation.html#january-1"><i class="fa fa-check"></i><b>2.6.1</b> January<span></span></a></li>
<li class="chapter" data-level="2.6.2" data-path="eda-precipitation.html"><a href="eda-precipitation.html#february-1"><i class="fa fa-check"></i><b>2.6.2</b> February<span></span></a></li>
<li class="chapter" data-level="2.6.3" data-path="eda-precipitation.html"><a href="eda-precipitation.html#march-1"><i class="fa fa-check"></i><b>2.6.3</b> March<span></span></a></li>
<li class="chapter" data-level="2.6.4" data-path="eda-precipitation.html"><a href="eda-precipitation.html#april-1"><i class="fa fa-check"></i><b>2.6.4</b> April<span></span></a></li>
<li class="chapter" data-level="2.6.5" data-path="eda-precipitation.html"><a href="eda-precipitation.html#may-1"><i class="fa fa-check"></i><b>2.6.5</b> May<span></span></a></li>
<li class="chapter" data-level="2.6.6" data-path="eda-precipitation.html"><a href="eda-precipitation.html#june-1"><i class="fa fa-check"></i><b>2.6.6</b> June<span></span></a></li>
<li class="chapter" data-level="2.6.7" data-path="eda-precipitation.html"><a href="eda-precipitation.html#july-1"><i class="fa fa-check"></i><b>2.6.7</b> July<span></span></a></li>
<li class="chapter" data-level="2.6.8" data-path="eda-precipitation.html"><a href="eda-precipitation.html#august-1"><i class="fa fa-check"></i><b>2.6.8</b> August<span></span></a></li>
<li class="chapter" data-level="2.6.9" data-path="eda-precipitation.html"><a href="eda-precipitation.html#september-1"><i class="fa fa-check"></i><b>2.6.9</b> September<span></span></a></li>
<li class="chapter" data-level="2.6.10" data-path="eda-precipitation.html"><a href="eda-precipitation.html#october-1"><i class="fa fa-check"></i><b>2.6.10</b> October<span></span></a></li>
<li class="chapter" data-level="2.6.11" data-path="eda-precipitation.html"><a href="eda-precipitation.html#november-1"><i class="fa fa-check"></i><b>2.6.11</b> November<span></span></a></li>
<li class="chapter" data-level="2.6.12" data-path="eda-precipitation.html"><a href="eda-precipitation.html#december-1"><i class="fa fa-check"></i><b>2.6.12</b> December<span></span></a></li>
<li><a href="eda-precipitation.html#section-1"></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="glyph-plots.html"><a href="glyph-plots.html"><i class="fa fa-check"></i><b>3</b> Glyph plots<span></span></a></li>
<li class="chapter" data-level="4" data-path="correlation-analysis.html"><a href="correlation-analysis.html"><i class="fa fa-check"></i><b>4</b> Correlation analysis<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="correlation-analysis.html"><a href="correlation-analysis.html#short-recap"><i class="fa fa-check"></i><b>4.1</b> Short Recap<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="correlation-analysis.html"><a href="correlation-analysis.html#correlation-of-sea-surface-temperature-and-precipitation"><i class="fa fa-check"></i><b>4.2</b> Correlation of Sea Surface Temperature and Precipitation<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="correlation-analysis.html"><a href="correlation-analysis.html#original-data"><i class="fa fa-check"></i><b>4.2.1</b> Original Data<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="correlation-analysis.html"><a href="correlation-analysis.html#deseasonalised-data"><i class="fa fa-check"></i><b>4.2.2</b> Deseasonalised Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="correlation-analysis.html"><a href="correlation-analysis.html#summary"><i class="fa fa-check"></i><b>4.3</b> Summary<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="correlation-analysis.html"><a href="correlation-analysis.html#original-data-1"><i class="fa fa-check"></i><b>4.3.1</b> Original Data<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="correlation-analysis.html"><a href="correlation-analysis.html#deseasonalised-data-1"><i class="fa fa-check"></i><b>4.3.2</b> Deseasonalised Data<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="main-idea-clustering.html"><a href="main-idea-clustering.html"><i class="fa fa-check"></i><b>5</b> Main Idea Clustering<span></span></a></li>
<li class="chapter" data-level="6" data-path="clustering-methods.html"><a href="clustering-methods.html"><i class="fa fa-check"></i><b>6</b> Clustering Methods<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="clustering-methods.html"><a href="clustering-methods.html#k-means"><i class="fa fa-check"></i><b>6.1</b> K-means<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="clustering-methods.html"><a href="clustering-methods.html#kmeans-characteristics"><i class="fa fa-check"></i><b>6.1.1</b> Kmeans characteristics<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="clustering-methods.html"><a href="clustering-methods.html#k-medoids"><i class="fa fa-check"></i><b>6.2</b> K-medoids<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="clustering-methods.html"><a href="clustering-methods.html#k-medoids-characteristics"><i class="fa fa-check"></i><b>6.2.1</b> K-medoids characteristics<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clustering-methods.html"><a href="clustering-methods.html#pca"><i class="fa fa-check"></i><b>6.3</b> PCA<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="clustering-methods.html"><a href="clustering-methods.html#gap-statistic"><i class="fa fa-check"></i><b>6.4</b> Gap statistic<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering-results.html"><a href="clustering-results.html"><i class="fa fa-check"></i><b>7</b> Clustering results<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering-results.html"><a href="clustering-results.html#k-means-and-pam-gap-statistics-without-pca"><i class="fa fa-check"></i><b>7.1</b> K-means and PAM gap statistics without PCA<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering-results.html"><a href="clustering-results.html#scree-plot"><i class="fa fa-check"></i><b>7.1.1</b> Scree plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering-results.html"><a href="clustering-results.html#k-means-and-pam-gap-statistics-after-applying-pca"><i class="fa fa-check"></i><b>7.2</b> K-means and PAM gap statistics after applying PCA<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="clustering-results.html"><a href="clustering-results.html#summary-1"><i class="fa fa-check"></i><b>7.3</b> Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="analyse-clustering-results.html"><a href="analyse-clustering-results.html"><i class="fa fa-check"></i><b>8</b> Analyse clustering results<span></span></a></li>
<li><a href="references.html#references">References<span></span></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Predicting Droughts in the Amazon Basin based on Global Sea Surface Temperatures</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering-methods" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Clustering Methods<a href="clustering-methods.html#clustering-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="k-means" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> K-means<a href="clustering-methods.html#k-means" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the following we briefly describe the K-means procedure.
Beforehand we have to specify a number of clusters <span class="math inline">\(C\)</span> we believe exist in our data. Then we randomly initialize <span class="math inline">\(C\)</span> cluster centers in the feature space.
Now two steps are repeated until convergence:</p>
<ol style="list-style-type: decimal">
<li>For each center we identify the points that are the closest to this center.<br />
These points “belong” now to a cluster <span class="math inline">\(C\)</span>.</li>
<li>In each cluster we compute the mean of each variable and get a vector of means.
This mean vector is now the new center of the cluster.</li>
</ol>
<p>As a measure of dissimilarity we use the Euclidean distance:</p>
<p><span class="math display">\[ d(x_i,x_{i´}) = \sum_{i=1}^p(x_{ij}-x_{i´j})^2=||x_i-x_{i´}||^2 \]</span>
Meaning for the points <span class="math inline">\(i\)</span> and <span class="math inline">\(i´\)</span> we compute the squared difference for each
variable and sum them up.
As stated above we are searching for clusters that are
themselves compact, meaning homogeneous.
We do so by minimizing the mean scatter inside the clusters.
We summarize this scatter as within-sum-of-squares</p>
<p><span class="math display">\[ W(C) = \frac{1}{2} \sum_{k=1}^{K} \sum_{C(i)=k} \sum_{C(i´)=k} ||x_i-x_{i´}||^2 \\
        = \sum_{k=1}^K N_k \sum_{C(i)=k} ||x_i-\bar{x}_k ||^2
\]</span></p>
<p>where <span class="math inline">\(\bar{x} = (\bar{x}_{1k},...,\bar{x}_{pk})\)</span> stands for the mean vectors of the <span class="math inline">\(k\)</span>th
cluster and <span class="math inline">\(N_k = \sum_{i=1}^N I(C(i)=k)\)</span>.</p>
<div id="kmeans-characteristics" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Kmeans characteristics<a href="clustering-methods.html#kmeans-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>variance of each distribution of each
attribute (variable) is spherical, variance is
symmetrical?
all variables have same variance, not the case
in our example, therefore scaling or pca
equal number of observations in each clusters, we don`t know</p>
<p>Since we use the Euclidean distance the similarity measures will be sensitive to outliers and scale.
K-means assumes that the variance of a variable´s distribution is spherical, meaning it wight not work well in situations that violate this assumptions (f.e non-spherical data). Further assumptions are same variance of the variables, and equally sized clusters.
Now how “large” these violations have to be so that
k-means does not work well anymore has no clear-cut answer.</p>
</div>
</div>
<div id="k-medoids" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> K-medoids<a href="clustering-methods.html#k-medoids" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can adjust k-means procedure so that we can use other distances
than the Euclidean distance. The only part of the k-means algorithm
that uses Euclidean distance is when we compute the cluster centers.
We can replace this step by formalizing an optimization with respect
to the cluster members. For example so that each center
has to be one of the observations assigned to the cluster. K-medoids is far more computationally intensive than K-means.</p>
<div id="k-medoids-characteristics" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> K-medoids characteristics<a href="clustering-methods.html#k-medoids-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>K-medoids is less sensitive to outliers, because it minimizes sum of pairwise dissimilarities instead of sum of squared Euclidean distances. As stated above it is also more computationally intensive.</p>
</div>
</div>
<div id="pca" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> PCA<a href="clustering-methods.html#pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Goal is reduction of correlated and eventually large number p variables to a few.
We accomplish this by creating new variables that are linear combinations of the
original ones. We call these new variables principle components.
The new variables are not correlated any more and ordered according
to the variance they explain. The first <span class="math inline">\(k &lt; p\)</span> principal components then contain the majority
of variance (<span class="citation">Fahrmeir et al. (<a href="#ref-fahrmeir1996multivariate" role="doc-biblioref">1996</a>)</span>).
As they are ordered they also provide a sequence of best linear approximations
of our data.
Let <span class="math inline">\(x_1, x_2,...,x_N\)</span>, be our observations and we represent them by a rank-q linear
model</p>
<p><span class="math display">\[ f(\lambda) = \mu + V_q\lambda\]</span></p>
<p>with <span class="math inline">\(\mu\)</span> a location vector in <span class="math inline">\(\mathbb{R}^p\)</span> and <span class="math inline">\(V_q\)</span> is a <span class="math inline">\(p \times q\)</span> matrix
with columns being orthogonal unit vectors as columns. <span class="math inline">\(\lambda\)</span> is a <span class="math inline">\(q\)</span> vector
of parameters.
In other words we are trying to fit a hyperplane of rank q to the data.
If we fit this model to minimize reconstruction error using least squares we solve</p>
<p><span class="math display">\[ \min_{\mu, \{\lambda_i\}, V_q } \sum_{i=1}^N||x_i - \mu - V_q\lambda_i ||^2 \]</span></p>
<p>If we partially optimize for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lambda_i\)</span> we obtain</p>
<p><span class="math display">\[ \hat{\mu} = \bar{x},\]</span>
<span class="math display">\[\hat{\lambda_i} = V_q^T(x_i - \bar{x}) \]</span>
Therefore we need to search for the orthogonal matrix <span class="math inline">\(V_q\)</span></p>
<p><span class="math display">\[ \min_{V_q} \sum_{i=1}^N ||(x_i-\bar{x}) - V_qV_q^t(x_i-\bar{x})||^2 \]</span></p>
<p>We can assume here that <span class="math inline">\(\bar{x}\)</span> is 0, if this not the case we can simply
center the observations <span class="math inline">\(\tilde{x}_i = x_i - \bar{x}\)</span>.
<span class="math inline">\(H_q = V_qV_q^T\)</span> projects each observation <span class="math inline">\(x_i\)</span> from the
original feature space onto the subspace that is spanned by the columns of <span class="math inline">\(V_q\)</span>.
<span class="math inline">\(H_qx_i\)</span> is then the orthogonal projection of <span class="math inline">\(x_i\)</span>
Hence <span class="math inline">\(H_q\)</span> is also called the <em>projection matrix</em>.</p>
<p>We find <span class="math inline">\(V_q\)</span> then by constructing the <em>singular value decomposition</em> of our data matrix X.
<span class="math inline">\(X\)</span> contains the (centered) observations in rows, giving a <span class="math inline">\(N \times p\)</span> matrix.
The SVD is then:</p>
<p><span class="math display">\[ X = UDV^T \]</span></p>
<p>Where <span class="math inline">\(U\)</span> is an orthogonal matrix containing the <em>left singular vectors</em> <span class="math inline">\(u_j\)</span> as columns, and <span class="math inline">\(V\)</span> contains the <em>right singular vectors</em> <span class="math inline">\(v_j\)</span>. The columns of <span class="math inline">\(U\)</span> span the columns space of <span class="math inline">\(X\)</span> and the columns of <span class="math inline">\(V\)</span> span the row space. <span class="math inline">\(D\)</span> is diagonal matrix which contains <em>singular values</em>, <span class="math inline">\(d_1 \leq d_2 \leq ... \leq d_p \leq 0\)</span>. <em>Singular values</em> are square roots of non-negative eigenvalues.
The columns of <span class="math inline">\(UD\)</span> are the principal components of <span class="math inline">\(X\)</span>.
So the SVD gives us the matrix <span class="math inline">\(V\)</span> (the first <span class="math inline">\(q\)</span> columns give the solution to the minimization problem above) as well as the principal components from <span class="math inline">\(UD\)</span> (<span class="citation">Hastie et al. (<a href="#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span>).</p>
</div>
<div id="gap-statistic" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Gap statistic<a href="clustering-methods.html#gap-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The idea of the gap statistic was introduced by <span class="citation">Tibshirani, Walther, and Hastie (<a href="#ref-tibshirani2001estimating" role="doc-biblioref">2001</a>)</span>
As stated above we usually measure how compact our clusters are
by assessing <span class="math inline">\(W(C)\)</span> or <span class="math inline">\(log(W_c)\)</span>. Where low values indicate compact clusters. To compare the value then, we need a reference.
We therefore want to estimate how large <span class="math inline">\(W_c\)</span> were if there were
no clusters present in our data.
The larger the difference between the <span class="math inline">\(W_c\)</span> from the data and
the one from the reference the more likely we are to say that
the found number of clusters is indeed correct.
We construct reference data by sampling from a uniform distribution
based on our data. Say we have <span class="math inline">\(p\)</span> variables. We sample <span class="math inline">\(n\)</span> times from
each of the <span class="math inline">\(p\)</span> uniformly distributed variables, where maximum and
minimum are obtained from our data.
We then cluster the reference data in the same we cluster our observed data and compute <span class="math inline">\(W_c\)</span>. We repeat this process several times and compute the average of <span class="math inline">\(W_c\)</span>, <span class="math inline">\(E \{log(W_c)\}\)</span>.
The gap statistic is then the difference
<span class="math display">\[E \{log(W_c) \} - log(W_c)\]</span>.
So in cases where our data is formed of clusters we would expect
a high gap statistic.
As <span class="citation">Tibshirani, Walther, and Hastie (<a href="#ref-tibshirani2001estimating" role="doc-biblioref">2001</a>)</span> note and as it is also done in the used R function
clusgap, doing a PCA on the data and compute the gap statistic on the
PCA scores can improves the results of gap statistic.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-fahrmeir1996multivariate" class="csl-entry">
Fahrmeir, Ludwig, Wolfgang Brachinger, Alfred Hamerle, and Gerhard Tutz. 1996. <em>Multivariate Statistische Verfahren</em>. Walter de Gruyter.
</div>
<div id="ref-hastie2009elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer.
</div>
<div id="ref-tibshirani2001estimating" class="csl-entry">
Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. <span>“Estimating the Number of Clusters in a Data Set via the Gap Statistic.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (2): 411–23.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="main-idea-clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering-results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
